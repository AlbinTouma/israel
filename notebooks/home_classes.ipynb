{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a4d5f6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import json\n",
    "import undetected_chromedriver as uc\n",
    "import time\n",
    "from random import randint\n",
    "import dataclasses as dc\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dd2002",
   "metadata": {},
   "source": [
    "## General functions\n",
    "\n",
    "Here we define a general function to scroll pages and detect type of article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f682976a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 15:44:26,791 - INFO - patching driver executable /home/albin/.local/share/undetected_chromedriver/undetected_chromedriver\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pressing button\n",
      "title.text Nakba survivor says Israeli ‘brutality’ worse than 77 years ago\n",
      "Pressing button\n",
      "title.text Ben & Jerry’s cofounder arrested at US Senate after protesting war in Gaza\n",
      "Pressing button\n",
      "title.text Photos: Israel attacks Gaza’s European Hospital\n",
      "Pressing button\n",
      "title.text Updates: Gaza people ‘deserve a much better future’, says Trump\n",
      "Pressing button\n",
      "title.text Hamas says it will release US-Israeli captive Edan Alexander\n",
      "Pressing button\n",
      "title.text Anger in Israel as Netanyahu’s war choices threaten captives’ fate in Gaza\n",
      "Pressing button\n",
      "title.text One killed, eight wounded as Israel hits Lebanon in major post-truce attack\n",
      "Pressing button\n",
      "title.text Israeli attacks kill more than 60 as Gaza blockade accelerates starvation\n",
      "Pressing button\n",
      "title.text The Take: Why was a Gaza ‘Freedom Flotilla’ ship attacked?\n",
      "Pressing button\n",
      "title.text The Take: Why was a Gaza ‘Freedom Flotilla’ ship attacked?\n",
      "Pressing button\n",
      "title.text How do you keep going in Gaza when everything tells you to stop?\n",
      "Pressing button\n",
      "title.text US man sentenced to 53 years for the murder of a Palestinian American child\n",
      "Pressing button\n",
      "title.text How activists and immigrants in the US can protect their privacy\n",
      "Pressing button\n",
      "title.text Updates: Israel kills at least 18 Palestinians in Gaza over last 24 hours\n",
      "Pressing button\n",
      "title.text UN Secretary-General: Two-state solution ‘near a point of no return’\n",
      "Pressing button\n",
      "title.text Why Israel’s Gaza tactics are back before international judges\n",
      "Pressing button\n",
      "title.text Thirteen killed, dozens under rubble as Israel bombs Gaza amid food crisis\n",
      "Pressing button\n",
      "title.text Israel is using high-powered bombs to maximise deaths in Gaza, experts say\n",
      "Pressing button\n",
      "title.text Gaza is burning. UK NGOs must abandon failed diplomacy and fight back\n",
      "Pressing button\n",
      "title.text Pope Francis spoke up for Palestinians until the end\n",
      "Pressing button\n",
      "title.text The Hawaii of Israel: How Trump legitimised a longstanding Israeli vision\n",
      "Pressing button\n",
      "title.text The Hawaii of Israel: How Trump legitimised a longstanding Israeli vision\n",
      "Pressing button\n",
      "title.text Hamas accuses Israel of weaponising aid as Gaza’s hunger crisis worsens\n",
      "Pressing button\n",
      "title.text Gaza ‘mass grave’ for Palestinians and those helping them: MSF\n",
      "Pressing button\n",
      "title.text Police use batons against pro-Palestinian protesters in Amsterdam\n",
      "Pressing button\n",
      "title.text Photos: Bethlehem carvers fret over second Easter without tourists\n",
      "Pressing button\n",
      "title.text France can and must do more than recognising Palestinian statehood\n",
      "Pressing button\n",
      "title.text UN: 36 recent Israeli strikes on Gaza killed only women, children\n",
      "Pressing button\n",
      "title.text Netanyahu backs dismissal of Israeli reservists calling for end to Gaza war\n",
      "Pressing button\n",
      "title.text Israeli ‘massacre’ in Gaza’s Shujayea as residential tower targeted\n",
      "Pressing button\n",
      "title.text Palestinians mourn journalist burned alive in Israeli strike on Gaza tent\n",
      "Pressing button\n",
      "title.text Palestinians mourn journalist burned alive in Israeli strike on Gaza tent\n",
      "Pressing button\n",
      "title.text Red Crescent demands international probe into Israel killing of Gaza medics\n",
      "Pressing button\n",
      "title.text Pro-Palestinian protesters interrupt Microsoft’s 50th anniversary event\n",
      "Pressing button\n",
      "title.text Gaza faces ‘largest orphan crisis’ in modern history, report says\n",
      "Pressing button\n",
      "title.text In Keir Starmer’s constituency, tensions flare over pro-Palestine activism\n",
      "Pressing button\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 360\u001b[39m\n\u001b[32m    355\u001b[39m \u001b[38;5;66;03m# Add the handler to the logger\u001b[39;00m\n\u001b[32m    356\u001b[39m logger.addHandler(file_handler)\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m \u001b[43mAljazeeraScraper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m#IsraeliTimesScraper().run()\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 342\u001b[39m, in \u001b[36mAljazeeraScraper.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    341\u001b[39m     homepage = WebPage(link=\u001b[33m'\u001b[39m\u001b[33mhttps://www.aljazeera.com/tag/israel-palestine-conflict/\u001b[39m\u001b[33m'\u001b[39m, media_type=\u001b[33m'\u001b[39m\u001b[33mhomepage\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscraper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhomepage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect_page_titles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43maljazeera_links\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincremental\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    343\u001b[39m     time.sleep(randint(\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 125\u001b[39m, in \u001b[36mScraper.run\u001b[39m\u001b[34m(self, scrape_object, scraper_function, filename, incremental)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m.driver.get(scrape_object.link)\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m incremental:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscroll_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscraper_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscraper_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscrape_object\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscrape_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    133\u001b[39m     \u001b[38;5;28mself\u001b[39m.scroll_method(driver=\u001b[38;5;28mself\u001b[39m.driver)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mScraper.scroll_method\u001b[39m\u001b[34m(self, driver, scraper_function, scrape_object, filename)\u001b[39m\n\u001b[32m     33\u001b[39m driver.execute_script(\u001b[33m\"\u001b[39m\u001b[33mwindow.scrollTo(0, document.body.scrollHeight);\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m time.sleep(\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m button_clicked = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclick_show_more_button\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m button_clicked:\n\u001b[32m     38\u001b[39m     button_pressed_count += \u001b[32m1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mScraper.click_show_more_button\u001b[39m\u001b[34m(self, driver)\u001b[39m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m button.is_displayed():\n\u001b[32m     18\u001b[39m         button.click()\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m         time.sleep(\u001b[32m1\u001b[39m)\n\u001b[32m     20\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "@dc.dataclass\n",
    "class WebPage:\n",
    "    website: str = None\n",
    "    url: str = None\n",
    "    link: str = None\n",
    "    title: str = None\n",
    "    media_type: str = None\n",
    "    date: str =None\n",
    "    content: str = None\n",
    "   \n",
    "class Scraper:\n",
    "\n",
    "    def click_show_more_button(self, driver) -> bool:\n",
    "        try:\n",
    "            print('Pressing button')\n",
    "            button = driver.find_element(By.XPATH, '//button[@class=\"show-more-button big-margin\"]')\n",
    "            if button.is_displayed():\n",
    "                button.click()\n",
    "                time.sleep(1)\n",
    "                return True\n",
    "        \n",
    "        except Exception as e:\n",
    "            pass\n",
    "        return False\n",
    "            \n",
    "    def scroll_method(self, driver, scraper_function=None, scrape_object=None, filename=None):\n",
    "        try:\n",
    "            last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            same_height_count = 0\n",
    "            button_pressed_count = 0\n",
    "\n",
    "            while True:\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(1)\n",
    "\n",
    "                button_clicked = self.click_show_more_button(driver)\n",
    "                if button_clicked:\n",
    "                    button_pressed_count += 1\n",
    "\n",
    "                if scraper_function and scrape_object:\n",
    "                    result, stop_flag = scraper_function(driver, scrape_object)\n",
    "                    self.write_to_jsonl(result, filename)\n",
    "                    if stop_flag:\n",
    "                        logger.info(\"Scroll end criteria met\")\n",
    "                        break\n",
    "\n",
    "    \n",
    "                new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            \n",
    "                if new_height == last_height:\n",
    "                    same_height_count += 1\n",
    "                    if same_height_count >= 3:\n",
    "                        logger.info(\"Reached the end of the page.\")\n",
    "                        break\n",
    "                else:\n",
    "                    same_height_count = 0\n",
    "                    last_height = new_height\n",
    "                    \n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "\n",
    "        \n",
    "    def write_to_jsonl(self, result: list[WebPage] | WebPage | Exception, filename: str):\n",
    "        \"\"\"Writes the result of the scraper function to a jsonl file. \n",
    "        If result is a list, each element is written to the file. \n",
    "        If result is a dict, it is written as a single line.\n",
    "        \"\"\"\n",
    "        CURRENT_DIR = Path().resolve()\n",
    "        PROJECT_ROOT = CURRENT_DIR.parent\n",
    "        #PROJECT_ROOT = Path(__file__).resolve().parent.parent  # Adjust as needed\n",
    "\n",
    "        output_dir = PROJECT_ROOT / 'output'\n",
    "        output_dir.mkdir(exist_ok=True)  # Create it if it doesn't exist\n",
    "        logs_dir = PROJECT_ROOT / 'logs'\n",
    "        logs_dir.mkdir(exist_ok=True)  # Create it if it doesn't exist\n",
    "\n",
    "        # Check if the file already exists\n",
    "        if not (output_dir / f'{filename}.jsonl').exists():\n",
    "            with open(f'{output_dir}/{filename}.jsonl', 'w') as f:\n",
    "                f.write('')\n",
    "                f.close()\n",
    "        if not (logs_dir / f'{filename}_error.jsonl').exists():\n",
    "            with open(f'{logs_dir}/{filename}_error.jsonl', 'w') as f:\n",
    "                f.write('')\n",
    "                f.close()\n",
    "        if not (logs_dir / f'{filename}_captcha.jsonl').exists():\n",
    "            with open(f'{logs_dir}/{filename}_captcha.jsonl', 'w') as f:\n",
    "                f.write('')\n",
    "                f.close()\n",
    "        \n",
    "\n",
    "\n",
    "        if result:\n",
    "            with open(f'{output_dir}/{filename}.jsonl', 'a') as f:        \n",
    "                if isinstance(result, list):\n",
    "                    result = [dc.asdict(r) for r in result]\n",
    "                    [f.write(json.dumps(r, ensure_ascii=False) + '\\n') for r in result]\n",
    "\n",
    "                elif isinstance(result, WebPage):\n",
    "                    f.write(json.dumps(dc.asdict(result), ensure_ascii=False) + '\\n')\n",
    "                    f.close()\n",
    "        \n",
    "                elif isinstance(result, Exception):\n",
    "                    with open(f'{logs_dir}/{filename}_error.jsonl', 'a') as f:\n",
    "                        f.write(json.dumps({'error': str(result)}, ensure_ascii=False) + '\\n')\n",
    "                        f.close()\n",
    "\n",
    "        else:\n",
    "            with open(f'{logs_dir}/{filename}_captcha.jsonl', 'a') as f:\n",
    "                if isinstance(result, list):\n",
    "                    for r in result:\n",
    "                        f.write(r + '\\n')\n",
    "                    f.close()\n",
    "                else:    \n",
    "                    f.write(result + '\\n')\n",
    "                    f.close()\n",
    "\n",
    "    def run(self, scrape_object: WebPage, scraper_function, filename: str, incremental: bool):\n",
    "        # This function is used to run the scraper\n",
    "        self.driver  = uc.Chrome(headless=False,use_subprocess=False)\n",
    "        self.driver.get(scrape_object.link)\n",
    "\n",
    "        if incremental:\n",
    "            self.scroll_method(\n",
    "                driver=self.driver,\n",
    "                scraper_function=scraper_function,\n",
    "                scrape_object=scrape_object,\n",
    "                filename=filename\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.scroll_method(driver=self.driver)\n",
    "            result =  scraper_function(self.driver, scrape_object)\n",
    "            self.write_to_jsonl(result, filename)\n",
    "        \n",
    "        self.driver.quit()\n",
    "\n",
    "class Database:\n",
    "    def check_if_exists(link: str) -> bool:\n",
    "        # This function is used to check if the link already exists in the database\n",
    "        with open('output/israeli_times_links.jsonl', 'r') as f:\n",
    "            for line in f:\n",
    "                if json.loads(line)['link'] == link:\n",
    "                    return True\n",
    "        return False    \n",
    "\n",
    "\n",
    "class IsraeliTimesScraper():\n",
    "    def __init__(self):\n",
    "        self.scraper = Scraper()\n",
    "\n",
    "    def detect_type_article(self, link: str) -> str:\n",
    "        # This function is used to detect the type of article\n",
    "        if 'liveblog' in link:\n",
    "            return 'liveblog'\n",
    "        elif 'blogs.timesofisrael.com' in link:\n",
    "            return 'blog'\n",
    "        elif 'https://jewishchronicle.timesofisrael.com/' in link:\n",
    "            return 'jewishchronicle'\n",
    "        else:\n",
    "            return 'article'\n",
    "\n",
    "    def collect_page_titles(self, driver, _: WebPage) -> list[WebPage]:\n",
    "        result = []\n",
    "        h = driver.find_elements(By.XPATH, '//div[@class=\"headline\"]/a')\n",
    "        already_scraped_count =  0\n",
    "        for link in h:\n",
    "            href = link.get_attribute('href')\n",
    "            if Database.check_if_exists(href):\n",
    "                already_scraped_count += 1\n",
    "                continue\n",
    "\n",
    "            type_of_article = self.detect_type_article(href)\n",
    "\n",
    "            result.append(\n",
    "                WebPage(\n",
    "                website='timesofisrael',\n",
    "                url=driver.current_url,\n",
    "                date=None,\n",
    "                title=link.text,\n",
    "                link=href,\n",
    "                media_type=type_of_article,\n",
    "                content=None\n",
    "            )\n",
    "            )\n",
    "        unique_domains = []\n",
    "        unique = set()\n",
    "        for r in result:\n",
    "            if r.link not in unique:\n",
    "                unique.add(r.link)\n",
    "                unique_domains.append(r)\n",
    "        print(\"Already scraped:\", already_scraped_count)\n",
    "        \n",
    "        if len(result) == 0:\n",
    "            return Exception('Already scraped all articles on this page')\n",
    "        \n",
    "        print('Collecting:', len(result), 'articles from the page')\n",
    "\n",
    "        return unique_domains\n",
    "\n",
    "    def scrape_article(self, driver, scrape_object: WebPage) -> WebPage:\n",
    "        \"\"\"Scrapes title, content, date from the article page.\"\"\"\n",
    "        try:\n",
    "            title = driver.find_element(By.XPATH, '//h1[@class=\"headline\"]').text\n",
    "            content = driver.find_element(By.XPATH, '//div[@class=\"the-content\"]').text\n",
    "            date = driver.find_element(By.XPATH, '//span[@class=\"date\"]').text\n",
    "            article = WebPage(\n",
    "                website='timesofisrael',\n",
    "                title =title,\n",
    "                date=date,\n",
    "                link=scrape_object.link,\n",
    "                media_type=scrape_object.media_type,\n",
    "                content=content\n",
    "            )\n",
    "\n",
    "            if \"Today\" in article.date:\n",
    "                new_date = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "                article.date = new_date\n",
    "\n",
    "            return article\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"'Error': {e}, 'title', {scrape_object.title}, 'link', {scrape_object.link} \\n\"\n",
    "\n",
    "    def collect_liveblog(self, driver, scrape_object: WebPage) -> list[WebPage]:\n",
    "        \"\"\"Scrapes title, content, date from the liveblog page.\"\"\"\n",
    "        try:\n",
    "            title = driver.find_elements(By.XPATH, '//div[@class=\"liveblog-paragraph\"]//h4')\n",
    "            content = driver.find_elements(By.XPATH, '//div[@class=\"liveblog-paragraph\"]//p')\n",
    "            href = driver.find_elements(By.XPATH, '//div[@class=\"liveblog-paragraph\"]//h4//a')\n",
    "            dates = driver.find_elements(By.XPATH, '//div[@class=\"liveblog-date\"]//a//span')\n",
    "            result = []\n",
    "            print(\n",
    "                \"title:\", len(title), \n",
    "                \"content:\", len(content), \n",
    "                \"link:\", len(href), \n",
    "                \"date:\", len(dates)\n",
    "                )\n",
    "\n",
    "            for t, i, h, d in zip(title, content, href, dates): \n",
    "                \n",
    "                # Convert epoch in timestamp to datetime\n",
    "                title = t.text\n",
    "                content = ''.join(i.text)\n",
    "                href = h\n",
    "                timestamp = int(d.get_attribute('data-timestamp'))\n",
    "                dt_object = datetime.datetime.utcfromtimestamp(timestamp)\n",
    "                epoch = dt_object.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            \n",
    "                result.append(WebPage(\n",
    "                    website='timesofisrael',\n",
    "                    url=driver.current_url,\n",
    "                    title =title,\n",
    "                    date=epoch,\n",
    "                    link=href.get_attribute('href'),\n",
    "                    media_type='liveblog',\n",
    "                    content=content\n",
    "                ))\n",
    "            \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return f\"'Error': {e}, 'title', {scrape_object.title}, 'link', {scrape_object.link} \\n\"\n",
    "\n",
    "    def collect_blogs(self, driver, scrape_object: WebPage) -> WebPage:\n",
    "        \"\"\"Scrapes title, content, date from the blog page.\"\"\"\n",
    "        try:\n",
    "            title = driver.find_element(By.XPATH, '//h1[@class=\"headline\"]').text\n",
    "            content = driver.find_element(By.XPATH, '//div[@class=\"article-content\"]').text\n",
    "            date = driver.find_element(By.XPATH, '//aside[@class=\"block cols1\"]//div[@class=\"date\"]').text\n",
    "            article = WebPage(\n",
    "                website='timesofisrael',\n",
    "                title =title,\n",
    "                date=date,\n",
    "                link=scrape_object.link,\n",
    "                media_type=scrape_object.media_type,\n",
    "                content=content\n",
    "            )  \n",
    "            return article\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"'Error': {e}, 'title', {scrape_object.title}, 'link', {scrape_object.link} \\n\"\n",
    "\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        homepage = WebPage(link='https://www.timesofisrael.com/', media_type='homepage')\n",
    "        self.scraper.run(homepage, self.collect_page_titles, 'israeli_times_links')\n",
    "        with open('output/israeli_times_links.jsonl', 'r') as f:\n",
    "            for line in f:\n",
    "                page = json.loads(line)\n",
    "                page = WebPage(link=page['link'], media_type=page['media_type'])\n",
    "                if page.media_type == 'article':\n",
    "                    self.scraper.run(page, self.scrape_article, 'data')\n",
    "                elif page.media_type == 'liveblog':\n",
    "                    self.scraper.run(page, self.collect_liveblog, 'data')\n",
    "                elif page.media_type == 'blog':\n",
    "                    self.scraper.run(page, self.collect_blogs, 'data')\n",
    "                \n",
    "                \n",
    "                time.sleep(randint(1, 3))\n",
    "\n",
    "\n",
    "\n",
    "class AljazeeraScraper():\n",
    "    def __init__(self):\n",
    "        self.scraper = Scraper()\n",
    "\n",
    "    def collect_page_titles(self, driver, _: WebPage) -> list[WebPage]:\n",
    "        try:\n",
    "            result = []\n",
    "            titles = driver.find_elements(By.XPATH, '//h3[@class=\"gc__title\"]')\n",
    "            hrefs = driver.find_elements(By.XPATH, '//a[@class=\"u-clickable-card__link\"]')\n",
    "            for title, link in zip(titles, hrefs):\n",
    "                result.append(\n",
    "                    WebPage(\n",
    "                        website='aljazeera',\n",
    "                        url=driver.current_url,\n",
    "                        date=None,\n",
    "                        title=title.text,\n",
    "                        link=link.get_attribute('href'),\n",
    "                    )\n",
    "                )  \n",
    "\n",
    "                print(\"title.text\", title.text)       \n",
    "                if title.text == 'Israel intensifies Gaza war during Trump visit':\n",
    "                    print(\"title:\", title.text)\n",
    "                    logger.info(\"End condition met\")\n",
    "                    return result, True\n",
    "\n",
    "            return result, False\n",
    "        \n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Error:\", e)\n",
    "            return f\"'Error': {e}, 'title', {_.title}, 'link', {_.link} \\n\", False\n",
    "        \n",
    "                        \n",
    "    def run(self):\n",
    "        homepage = WebPage(link='https://www.aljazeera.com/tag/israel-palestine-conflict/', media_type='homepage')\n",
    "        self.scraper.run(homepage, self.collect_page_titles, 'aljazeera_links', incremental=True)\n",
    "        time.sleep(randint(1, 3))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "file_handler = logging.FileHandler('scraper.log')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "# Create a formatter and set it for the handler\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "# Add the handler to the logger\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "\n",
    "\n",
    "AljazeeraScraper().run()\n",
    "\n",
    "#IsraeliTimesScraper().run()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
