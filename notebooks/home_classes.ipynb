{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4d5f6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import json\n",
    "import undetected_chromedriver as uc\n",
    "import time\n",
    "from random import randint\n",
    "import dataclasses as dc\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dd2002",
   "metadata": {},
   "source": [
    "## General functions\n",
    "\n",
    "Here we define a general function to scroll pages and detect type of article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f682976a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached the end of the page.\n",
      "Collecting: 93 articles from the page\n",
      "Israel kills 132 Palestinians in Gaza, hospitals in north ‘out of service’ \n",
      "Poland votes in tight presidential election \n",
      " \n",
      "At least 10 reported killed in suicide bomb blast in Somalia’s Mogadishu \n",
      "Indian professor held over remarks on military operation against Pakistan \n",
      "Romanians cast ballots in tense presidential run-off \n",
      "Portugal holds its third elections in three years \n",
      "Children burnt to death in tents as Israel intensifies strikes across Gaza \n",
      "Mexican Navy ship crashes into New York’s Brooklyn Bridge, killing 2 people \n",
      "MUST READ \n",
      "Could AI help elderly people and refugees reconstruct unrecorded pasts? \n",
      "‘Exploding inequality’: The fight for the hearts and minds of Poland’s left \n",
      "Malaysia’s ‘fish hunters’ target invasive species, one catch at a time \n",
      "MORE HEADLINES \n",
      "Iran’s leaders slam Trump for ‘disgraceful’ remarks during Middle East tour \n",
      "Crystal Palace beat Man City 1-0 in FA Cup final at Wembley \n",
      "New Gaza truce talks under way as Israel expands ground assault \n",
      "OPINION \n",
      "Trump’s tariffs are failing, but the old model won’t save us either \n",
      "Israel’s Gaza ‘disengagement’ that paved the way for conquest \n",
      "WATCH LATEST VIDEOS \n",
      "Featured \n",
      "‘I dream of an all-African lineup at the Olympics’: Letsile Tebogo \n",
      "Breaking down a deadly week in Gaza as Israel kills hundreds \n",
      "Has India offered Trump zero tariffs? What we know and why it matters \n",
      "‘New paradigm’: A fractured Portugal votes again, amid corruption cloud \n",
      "Poland presidential election 2025: From migration to EU, what’s at stake? \n",
      "Week in Pictures: From deadly storms in US to forced displacement in Gaza \n",
      "Trump says he will call Putin, Zelenskyy to discuss Ukraine ‘bloodbath’ \n",
      "Opinion \n",
      "Arab League calls for funds to rebuild Gaza at summit in Baghdad \n",
      "FBI calls explosion near California fertility clinic an ‘act of terrorism’ \n",
      "Sport \n",
      "Thanks for the memories: Everton’s famous Goodison Park stages final game \n",
      "Knicks crush Celtics, reach conference finals for first time in 25 years \n",
      "‘Javelin is my career, not social media’: Neeraj Chopra on mental pressure \n",
      "Yamal helps Barcelona seal La Liga title at rivals Espanyol \n",
      "What to know about MLB’s decision to lift bans on Pete Rose, Joe Jackson \n",
      "‘Fight back’: Pedro Pascal urges Cannes to resist US political pressure \n",
      "At least three killed as Syrian forces raid ISIL hideouts in Aleppo \n",
      "Photos: Tens of thousands join worldwide protests to mark Palestinian Nakba \n",
      "At least 21 dead after storms hit US states of Missouri and Kentucky \n",
      "Seven European nations urge Israel to ‘reverse its current policy’ on Gaza \n",
      "Poland presidential election 2025: Polls, results, contenders \n",
      "Libyan ministers resign as protesters call for government to step down \n",
      "UK police charge three Iranians with suspected espionage \n",
      "US announces first ‘terrorism’ charges for supporting a Mexican cartel \n",
      "‘Xenophobic’: Neighbours outraged over Mauritania’s mass migrant pushback \n",
      "Moody’s strips US government of top credit rating \n",
      "Romania braces for heated presidential vote after controversial annulment \n",
      "Thousands of Palestinians flee north Gaza amid intensified Israeli attacks \n",
      "Did Pakistan shoot down five Indian fighter jets? What we know \n",
      "Russia-Ukraine war: List of key events, day 1,178 \n",
      "Five key takeaways from US President Donald Trump’s Middle East trip \n",
      "‘I cannot stand by’: Former ambassador denounces Ukraine shift under Trump \n",
      "How to achieve a lasting ceasefire between Russia and Ukraine? \n",
      "Israel launches strikes on two Yemen ports \n",
      "Istanbul talks highlight Turkiye’s balancing act between Russia and Ukraine \n",
      "Russia, Ukraine agree prisoner swap as talks end without major breakthrough \n",
      "Trump’s ‘big, beautiful bill’ at a crucial juncture \n",
      "US Supreme Court blocks the Trump administration’s use of Alien Enemies Act \n",
      "European leaders consult Trump to align response to Russia-Ukraine talks \n",
      "ICC prosecutor to step aside until probe into alleged misconduct ends \n",
      "Attacker who stabbed author Salman Rushdie sentenced to 25 years in prison \n",
      "Has Donald Trump taken US-Gulf relations to a new era? \n",
      "World Bank says Syria eligible for new loans after debts cleared \n",
      "Trump calls on Iran to ‘move quickly’ on nuclear proposal \n",
      "NJ Transit workers go on strike after wage increase talks stall \n",
      "Charter Communications to buy rival Cox for $21.9bn \n",
      "Was ex-FBI chief Comey’s ’86 47′ post calling for Trump assassination? \n",
      "Global hunger hits new high amid conflict, extreme weather: UN \n",
      "In surprise move Wegovy-maker Novo Nordisk ousts CEO amid sagging sales \n",
      "What is famine, and why is Gaza at risk of reaching it soon? \n",
      "Peru arrests suspect in gold rush massacre \n",
      "Russia and Ukraine delegations meet for talks in Turkiye \n",
      "Influencer shot live on TikTok: How rampant is femicide in Mexico? \n",
      "Court rejects Australian soldier’s defamation appeal over Afghan killings \n",
      "Trump’s decision to lift Syria sanctions fuels dreams of economic revival \n",
      "Russia targeting journalists in Ukraine hotel strikes: Report \n",
      "Palestine before the Nakba, in 100 photos \n",
      "Gaza death toll passes 53,000 as Israel drives towards ‘conquest’ \n",
      "Israel kills 132 Palestinians in Gaza, hospitals in north ‘out of service’ \n",
      "Iran’s leaders slam Trump for ‘disgraceful’ remarks during Middle East tour \n",
      "Indian professor held over remarks on military operation against Pakistan \n",
      "Mexican Navy ship crashes into New York’s Brooklyn Bridge, killing 2 people \n",
      "Poland presidential election 2025: Polls, results, contenders \n",
      "‘I cannot stand by’: Former ambassador denounces Ukraine shift under Trump \n",
      "Romanians cast ballots in tense presidential run-off \n",
      "Malaysia’s ‘fish hunters’ target invasive species, one catch at a time \n",
      "Five key takeaways from US President Donald Trump’s Middle East trip \n",
      "Children burnt to death in tents as Israel intensifies strikes across Gaza \n",
      "Sign up for Al Jazeera \n",
      "We and our partners process data to provide: \n"
     ]
    }
   ],
   "source": [
    "@dc.dataclass\n",
    "class WebPage:\n",
    "    website: str = None\n",
    "    url: str = None\n",
    "    link: str = None\n",
    "    title: str = None\n",
    "    media_type: str = None\n",
    "    date: str =None\n",
    "    content: str = None\n",
    "   \n",
    "class Scraper:\n",
    "\n",
    "    def show_more_button(self, driver) -> bool:\n",
    "        try:\n",
    "            button = driver.find_element(By.XPATH, '//button[@class=\"show-more-button big-margin\"]')\n",
    "            if button.displayed():\n",
    "                button.click()\n",
    "                time.sleep(1)\n",
    "                return True\n",
    "        \n",
    "        except Exception as e:\n",
    "            pass\n",
    "        return False\n",
    "            \n",
    "    def scroll_method(self, driver, max_tries=10):\n",
    "        try:\n",
    "            last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            same_height_count = 0\n",
    "\n",
    "            for _ in range(max_tries):\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(1)\n",
    "\n",
    "                self.show_more_button(driver)\n",
    "\n",
    "                new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                \n",
    "                if new_height == last_height:\n",
    "                    same_height_count += 1\n",
    "                    if same_height_count >= 3:\n",
    "                        print(\"Reached the end of the page.\")\n",
    "                        break\n",
    "                else:\n",
    "                    same_height_count = 0\n",
    "                    last_height = new_height\n",
    "                    \n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "\n",
    "        \n",
    "    def write_to_jsonl(self, result: list[WebPage] | WebPage | Exception, filename: str):\n",
    "        \"\"\"Writes the result of the scraper function to a jsonl file. \n",
    "        If result is a list, each element is written to the file. \n",
    "        If result is a dict, it is written as a single line.\n",
    "        \"\"\"\n",
    "        CURRENT_DIR = Path().resolve()\n",
    "        PROJECT_ROOT = CURRENT_DIR.parent\n",
    "        #PROJECT_ROOT = Path(__file__).resolve().parent.parent  # Adjust as needed\n",
    "\n",
    "        output_dir = PROJECT_ROOT / 'output'\n",
    "        output_dir.mkdir(exist_ok=True)  # Create it if it doesn't exist\n",
    "        logs_dir = PROJECT_ROOT / 'logs'\n",
    "        logs_dir.mkdir(exist_ok=True)  # Create it if it doesn't exist\n",
    "\n",
    "        # Check if the file already exists\n",
    "        if not (output_dir / f'{filename}.jsonl').exists():\n",
    "            with open(f'{output_dir}/{filename}.jsonl', 'w') as f:\n",
    "                f.write('')\n",
    "                f.close()\n",
    "        if not (logs_dir / f'{filename}_error.jsonl').exists():\n",
    "            with open(f'{logs_dir}/{filename}_error.jsonl', 'w') as f:\n",
    "                f.write('')\n",
    "                f.close()\n",
    "        if not (logs_dir / f'{filename}_captcha.jsonl').exists():\n",
    "            with open(f'{logs_dir}/{filename}_captcha.jsonl', 'w') as f:\n",
    "                f.write('')\n",
    "                f.close()\n",
    "        \n",
    "\n",
    "\n",
    "        if result:\n",
    "            with open(f'{output_dir}/{filename}.jsonl', 'a') as f:        \n",
    "                if isinstance(result, list):\n",
    "                    result = [dc.asdict(r) for r in result]\n",
    "                    [f.write(json.dumps(r, ensure_ascii=False) + '\\n') for r in result]\n",
    "\n",
    "                elif isinstance(result, WebPage):\n",
    "                    f.write(json.dumps(dc.asdict(result), ensure_ascii=False) + '\\n')\n",
    "                    f.close()\n",
    "        \n",
    "                elif isinstance(result, Exception):\n",
    "                    with open(f'{logs_dir}/{filename}_error.jsonl', 'a') as f:\n",
    "                        f.write(json.dumps({'error': str(result)}, ensure_ascii=False) + '\\n')\n",
    "                        f.close()\n",
    "\n",
    "        else:\n",
    "            with open(f'{logs_dir}/{filename}_captcha.jsonl', 'a') as f:\n",
    "                if isinstance(result, list):\n",
    "                    for r in result:\n",
    "                        f.write(r + '\\n')\n",
    "                    f.close()\n",
    "                else:    \n",
    "                    f.write(result + '\\n')\n",
    "                    f.close()\n",
    "\n",
    "    def run(self, scrape_object: WebPage, scraper_function, filename: str):\n",
    "        # This function is used to run the scraper\n",
    "        self.driver  = uc.Chrome(headless=False,use_subprocess=False)\n",
    "        self.driver.get(scrape_object.link)\n",
    "        self.scroll_method(self.driver)\n",
    "        result =  scraper_function(self.driver, scrape_object)\n",
    "        self.write_to_jsonl(result, filename)\n",
    "        self.driver.quit()\n",
    "\n",
    "class Database:\n",
    "    def check_if_exists(link: str) -> bool:\n",
    "        # This function is used to check if the link already exists in the database\n",
    "        with open('output/israeli_times_links.jsonl', 'r') as f:\n",
    "            for line in f:\n",
    "                if json.loads(line)['link'] == link:\n",
    "                    return True\n",
    "        return False    \n",
    "\n",
    "\n",
    "class IsraeliTimesScraper():\n",
    "    def __init__(self):\n",
    "        self.scraper = Scraper()\n",
    "\n",
    "    def detect_type_article(self, link: str) -> str:\n",
    "        # This function is used to detect the type of article\n",
    "        if 'liveblog' in link:\n",
    "            return 'liveblog'\n",
    "        elif 'blogs.timesofisrael.com' in link:\n",
    "            return 'blog'\n",
    "        elif 'https://jewishchronicle.timesofisrael.com/' in link:\n",
    "            return 'jewishchronicle'\n",
    "        else:\n",
    "            return 'article'\n",
    "\n",
    "    def collect_page_titles(self, driver, _: WebPage) -> list[WebPage]:\n",
    "        result = []\n",
    "        h = driver.find_elements(By.XPATH, '//div[@class=\"headline\"]/a')\n",
    "        already_scraped_count =  0\n",
    "        for link in h:\n",
    "            href = link.get_attribute('href')\n",
    "            if Database.check_if_exists(href):\n",
    "                already_scraped_count += 1\n",
    "                continue\n",
    "\n",
    "            type_of_article = self.detect_type_article(href)\n",
    "\n",
    "            result.append(\n",
    "                WebPage(\n",
    "                website='timesofisrael',\n",
    "                url=driver.current_url,\n",
    "                date=None,\n",
    "                title=link.text,\n",
    "                link=href,\n",
    "                media_type=type_of_article,\n",
    "                content=None\n",
    "            )\n",
    "            )\n",
    "        unique_domains = []\n",
    "        unique = set()\n",
    "        for r in result:\n",
    "            if r.link not in unique:\n",
    "                unique.add(r.link)\n",
    "                unique_domains.append(r)\n",
    "        print(\"Already scraped:\", already_scraped_count)\n",
    "        \n",
    "        if len(result) == 0:\n",
    "            return Exception('Already scraped all articles on this page')\n",
    "        \n",
    "        print('Collecting:', len(result), 'articles from the page')\n",
    "\n",
    "        return unique_domains\n",
    "\n",
    "    def scrape_article(self, driver, scrape_object: WebPage) -> WebPage:\n",
    "        \"\"\"Scrapes title, content, date from the article page.\"\"\"\n",
    "        try:\n",
    "            title = driver.find_element(By.XPATH, '//h1[@class=\"headline\"]').text\n",
    "            content = driver.find_element(By.XPATH, '//div[@class=\"the-content\"]').text\n",
    "            date = driver.find_element(By.XPATH, '//span[@class=\"date\"]').text\n",
    "            article = WebPage(\n",
    "                website='timesofisrael',\n",
    "                title =title,\n",
    "                date=date,\n",
    "                link=scrape_object.link,\n",
    "                media_type=scrape_object.media_type,\n",
    "                content=content\n",
    "            )\n",
    "\n",
    "            if \"Today\" in article.date:\n",
    "                new_date = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "                article.date = new_date\n",
    "\n",
    "            return article\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"'Error': {e}, 'title', {scrape_object.title}, 'link', {scrape_object.link} \\n\"\n",
    "\n",
    "    def collect_liveblog(self, driver, scrape_object: WebPage) -> list[WebPage]:\n",
    "        \"\"\"Scrapes title, content, date from the liveblog page.\"\"\"\n",
    "        try:\n",
    "            title = driver.find_elements(By.XPATH, '//div[@class=\"liveblog-paragraph\"]//h4')\n",
    "            content = driver.find_elements(By.XPATH, '//div[@class=\"liveblog-paragraph\"]//p')\n",
    "            href = driver.find_elements(By.XPATH, '//div[@class=\"liveblog-paragraph\"]//h4//a')\n",
    "            dates = driver.find_elements(By.XPATH, '//div[@class=\"liveblog-date\"]//a//span')\n",
    "            result = []\n",
    "            print(\n",
    "                \"title:\", len(title), \n",
    "                \"content:\", len(content), \n",
    "                \"link:\", len(href), \n",
    "                \"date:\", len(dates)\n",
    "                )\n",
    "\n",
    "            for t, i, h, d in zip(title, content, href, dates): \n",
    "                \n",
    "                # Convert epoch in timestamp to datetime\n",
    "                title = t.text\n",
    "                content = ''.join(i.text)\n",
    "                href = h\n",
    "                timestamp = int(d.get_attribute('data-timestamp'))\n",
    "                dt_object = datetime.datetime.utcfromtimestamp(timestamp)\n",
    "                epoch = dt_object.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            \n",
    "                result.append(WebPage(\n",
    "                    website='timesofisrael',\n",
    "                    url=driver.current_url,\n",
    "                    title =title,\n",
    "                    date=epoch,\n",
    "                    link=href.get_attribute('href'),\n",
    "                    media_type='liveblog',\n",
    "                    content=content\n",
    "                ))\n",
    "            \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "            return f\"'Error': {e}, 'title', {scrape_object.title}, 'link', {scrape_object.link} \\n\"\n",
    "\n",
    "    def collect_blogs(self, driver, scrape_object: WebPage) -> WebPage:\n",
    "        \"\"\"Scrapes title, content, date from the blog page.\"\"\"\n",
    "        try:\n",
    "            title = driver.find_element(By.XPATH, '//h1[@class=\"headline\"]').text\n",
    "            content = driver.find_element(By.XPATH, '//div[@class=\"article-content\"]').text\n",
    "            date = driver.find_element(By.XPATH, '//aside[@class=\"block cols1\"]//div[@class=\"date\"]').text\n",
    "            article = WebPage(\n",
    "                website='timesofisrael',\n",
    "                title =title,\n",
    "                date=date,\n",
    "                link=scrape_object.link,\n",
    "                media_type=scrape_object.media_type,\n",
    "                content=content\n",
    "            )  \n",
    "            return article\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"'Error': {e}, 'title', {scrape_object.title}, 'link', {scrape_object.link} \\n\"\n",
    "\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        homepage = WebPage(link='https://www.timesofisrael.com/', media_type='homepage')\n",
    "        self.scraper.run(homepage, self.collect_page_titles, 'israeli_times_links')\n",
    "        with open('output/israeli_times_links.jsonl', 'r') as f:\n",
    "            for line in f:\n",
    "                page = json.loads(line)\n",
    "                page = WebPage(link=page['link'], media_type=page['media_type'])\n",
    "                if page.media_type == 'article':\n",
    "                    self.scraper.run(page, self.scrape_article, 'data')\n",
    "                elif page.media_type == 'liveblog':\n",
    "                    self.scraper.run(page, self.collect_liveblog, 'data')\n",
    "                elif page.media_type == 'blog':\n",
    "                    self.scraper.run(page, self.collect_blogs, 'data')\n",
    "                \n",
    "                \n",
    "                time.sleep(randint(1, 3))\n",
    "\n",
    "\n",
    "\n",
    "class AljazeeraScraper():\n",
    "    def __init__(self):\n",
    "        self.scraper = Scraper()\n",
    "\n",
    "    def collect_page_titles(self, driver, _: WebPage) -> list[WebPage]:\n",
    "        try:\n",
    "            result = []\n",
    "            h = driver.find_elements(By.XPATH, '//h3')\n",
    "            for link in h:\n",
    "                result.append(\n",
    "                    WebPage(\n",
    "                        website='aljazeera',\n",
    "                        url=driver.current_url,\n",
    "                        date=None,\n",
    "                        title=link.text,\n",
    "                        link='',\n",
    "                    )\n",
    "                ) \n",
    "            print('Collecting:', len(result), 'articles from the page')\n",
    "            for i in result:\n",
    "                print(i.title, i.link)        \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "            return f\"'Error': {e}, 'title', {_.title}, 'link', {_.link} \\n\"\n",
    "        \n",
    "                        \n",
    "    def run(self):\n",
    "        homepage = WebPage(link='https://www.aljazeera.com/', media_type='homepage')\n",
    "        self.scraper.run(homepage, self.collect_page_titles, 'aljazeera')\n",
    "        time.sleep(randint(1, 3))\n",
    "\n",
    "\n",
    "AljazeeraScraper().run()\n",
    "\n",
    "#IsraeliTimesScraper().run()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
