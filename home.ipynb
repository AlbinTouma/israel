{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a4d5f6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import json\n",
    "import undetected_chromedriver as uc\n",
    "import time\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f682976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_method():\n",
    "    # This method is used to scroll the page\n",
    "    last_height = 0\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        else:\n",
    "            last_height = new_height\n",
    "\n",
    "\n",
    "def detect_type_article(link: str) -> str:\n",
    "    # This function is used to detect the type of article\n",
    "    if 'liveblog' in link:\n",
    "        return 'liveblog'\n",
    "    elif 'blogs.timesofisrael.com' in link:\n",
    "        return 'blog'\n",
    "    elif 'https://jewishchronicle.timesofisrael.com/' in link:\n",
    "        return 'jewishchronicle'\n",
    "    else:\n",
    "        return 'article'\n",
    "\n",
    "def collect_page_titles() -> list[dict]:\n",
    "    result = []\n",
    "    h = driver.find_elements(By.XPATH, '//div[@class=\"headline\"]/a')\n",
    "    for link in h:\n",
    "        result.append({\n",
    "            'title': link.text, \n",
    "            'link': link.get_attribute('href'),\n",
    "            'type': detect_type_article(link.get_attribute('href')),\n",
    "        })\n",
    "    return result\n",
    "\n",
    "\n",
    "#driver  = webdriver.Chrome()\n",
    "#titles_to_scrape: list[dict] = collect_page_titles()\n",
    "#with open('result.jsonl', 'w') as f:\n",
    "#    for i in titles_to_scrape:\n",
    "#        f.write(json.dumps(i, ensure_ascii=False) + '\\n')\n",
    "#\n",
    "#    f.close()\n",
    "    \n",
    "#driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7054b1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped https://www.timesofisrael.com/israel-claims-its-promoting-palestinian-emigration-from-gaza-so-why-are-so-few-leaving/\n",
      "Scraped https://www.timesofisrael.com/liveblog-may-09-2025/\n",
      "Scraped https://www.timesofisrael.com/two-idf-soldiers-killed-six-wounded-in-southern-gaza-fighting/\n",
      "Scraped https://www.timesofisrael.com/israeli-plan-to-initially-only-feed-60-of-gazans-as-they-endure-extreme-deprivation/\n",
      "Scraped https://www.timesofisrael.com/leading-agency-shuts-its-gaza-soup-kitchens-amid-continued-israeli-aid-ban/\n",
      "Scraped https://www.timesofisrael.com/eu-to-review-trade-ties-with-israel-following-criticism-of-conduct-in-war-in-gaza/\n",
      "Scraped https://www.timesofisrael.com/us-pressuring-humanitarian-groups-to-get-behind-israeli-aid-plan-for-gaza/\n",
      "Scraped https://www.timesofisrael.com/israel-claims-its-promoting-palestinian-emigration-from-gaza-so-why-are-so-few-leaving/\n",
      "Scraped https://www.timesofisrael.com/liveblog-may-09-2025/\n",
      "Scraped https://www.timesofisrael.com/two-idf-soldiers-killed-six-wounded-in-southern-gaza-fighting/\n",
      "Scraped https://www.timesofisrael.com/israeli-plan-to-initially-only-feed-60-of-gazans-as-they-endure-extreme-deprivation/\n",
      "Scraped https://www.timesofisrael.com/leading-agency-shuts-its-gaza-soup-kitchens-amid-continued-israeli-aid-ban/\n",
      "Scraped https://www.timesofisrael.com/eu-to-review-trade-ties-with-israel-following-criticism-of-conduct-in-war-in-gaza/\n",
      "Scraped https://www.timesofisrael.com/us-pressuring-humanitarian-groups-to-get-behind-israeli-aid-plan-for-gaza/\n",
      "Scraped https://www.timesofisrael.com/huckabee-says-us-not-required-to-get-permission-from-israel-to-cut-deal-with-houthis/\n",
      "Scraped https://www.timesofisrael.com/after-relentless-us-bombing-campaign-yemens-houthis-are-biggest-victors-of-truce/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 59\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m dicts:\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i[\u001b[33m'\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33marticle\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m          \u001b[43mscraper_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscrape_article\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m     time.sleep(randint(\u001b[32m2\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m     62\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mScraped \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi[\u001b[33m'\u001b[39m\u001b[33mlink\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mscraper_wrapper\u001b[39m\u001b[34m(scrape_object, scraper_function)\u001b[39m\n\u001b[32m     37\u001b[39m driver  = uc.Chrome(headless=\u001b[38;5;28;01mFalse\u001b[39;00m,use_subprocess=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     38\u001b[39m driver.get(scrape_object[\u001b[33m'\u001b[39m\u001b[33mlink\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[43mscroll_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m result =  scraper_function(driver, scrape_object)\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mtest.jsonl\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33ma\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:    \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mscroll_method\u001b[39m\u001b[34m(driver)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m      9\u001b[39m     driver.execute_script(\u001b[33m\"\u001b[39m\u001b[33mwindow.scrollTo(0, document.body.scrollHeight);\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     time.sleep(\u001b[32m1\u001b[39m)\n\u001b[32m     11\u001b[39m     new_height = driver.execute_script(\u001b[33m\"\u001b[39m\u001b[33mreturn document.body.scrollHeight\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m new_height == last_height:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "## RIght now takes 40 min to scrape articles\n",
    "## TODO: add a function to slate captcha articles (null) for later\n",
    "## Looks like email pop up does not prevent scraping\n",
    "\n",
    "def scroll_method(driver):\n",
    "    # This method is used to scroll the page\n",
    "    last_height = 0\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        else:\n",
    "            last_height = new_height\n",
    "\n",
    "\n",
    "def scrape_article(driver, scrape_object: dict) -> dict:\n",
    "    driver.get(scrape_object['link'])\n",
    "    try:\n",
    "        title = driver.find_element(By.XPATH, '//h1[@class=\"headline\"]').text\n",
    "        content = driver.find_element(By.XPATH, '//div[@class=\"the-content\"]').text\n",
    "        date = driver.find_element(By.XPATH, '//span[@class=\"date\"]').text\n",
    "        return {\n",
    "            'title': title,\n",
    "            'date': date,\n",
    "            'link': scrape_object['link'],\n",
    "            'type': scrape_object['type'],\n",
    "            'content': content\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def scraper_wrapper(scrape_object: dict, scraper_function: callable):\n",
    "    driver  = uc.Chrome(headless=True,use_subprocess=False)\n",
    "    driver.get(scrape_object['link'])\n",
    "    scroll_method(driver)\n",
    "\n",
    "    result =  scraper_function(driver, scrape_object)\n",
    "    if result:\n",
    "        with open('test.jsonl', 'a') as f:    \n",
    "                f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n",
    "                f.close()\n",
    "    else:\n",
    "        with open('captchas.jsonl', 'a') as f:    \n",
    "                f.write(json.dumps(scrape_object, ensure_ascii=False) + '\\n')\n",
    "                f.close()\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "dicts = []\n",
    "with open('result.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        dicts.append(json.loads(line))\n",
    "    f.close()\n",
    "\n",
    "for i in dicts:\n",
    "    if i['type'] == 'article':\n",
    "         scraper_wrapper(i, scrape_article)\n",
    "    \n",
    "    time.sleep(randint(2, 5))\n",
    "    print(f\"Scraped {i['link']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "887a9011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2025-05-10 17:34:26\n",
      " \t Success rate: 72.0%\n",
      " \t Articles scraped: 293\n",
      " \t Articles with errors: 114\n",
      " \t Total number of articles: 407 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dicts = []\n",
    "with open('scaped_articles.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        dicts.append(json.loads(line))\n",
    "    f.close()\n",
    "\n",
    "count = 0\n",
    "for i in dicts:\n",
    "    if i:\n",
    "        count += 1\n",
    "print(\n",
    "      f'\\n {time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())}'\n",
    "      f'\\n \\t Success rate: {round(count/len(dicts)*100, 1)}%' \n",
    "      f'\\n \\t Articles scraped: {count}'\n",
    "      f'\\n \\t Articles with errors: {len(dicts) - count}'\n",
    "      f'\\n \\t Total number of articles: {len(dicts)} \\n'\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7622566",
   "metadata": {},
   "source": [
    "```   author = driver.find_element(By.XPATH, '//span[@class=\"byline\"]/a').text\n",
    "```\n",
    "Note to self that author can have several <a> elements if more than one author"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
