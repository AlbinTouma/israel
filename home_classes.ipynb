{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4d5f6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import json\n",
    "import undetected_chromedriver as uc\n",
    "import time\n",
    "from random import randint\n",
    "import dataclasses as dc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dd2002",
   "metadata": {},
   "source": [
    "## General functions\n",
    "\n",
    "Here we define a general function to scroll pages and detect type of article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f682976a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already scraped: 0\n",
      "Collecting: 457 articles from the page\n"
     ]
    }
   ],
   "source": [
    "@dc.dataclass\n",
    "class WebPage:\n",
    "    website: str = None\n",
    "    url: str = None\n",
    "    link: str = None\n",
    "    title: str = None\n",
    "    media_type: str = None\n",
    "    date: str=None\n",
    "    content: str = None\n",
    "\n",
    "\n",
    "\n",
    "class Scraper:\n",
    "        \n",
    "    def scroll_method(self, driver):\n",
    "        last_height = 0\n",
    "        while True:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(1)\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            else:\n",
    "                last_height = new_height\n",
    "\n",
    "    def write_to_jsonl(self, result: list[WebPage] | WebPage | Exception, filename: str):\n",
    "        \"\"\"Writes the result of the scraper function to a jsonl file. \n",
    "        If result is a list, each element is written to the file. \n",
    "        If result is a dict, it is written as a single line.\n",
    "        \"\"\"\n",
    "        if result:\n",
    "            with open(f'{filename}.jsonl', 'a') as f:        \n",
    "                if isinstance(result, list):\n",
    "                    result = [dc.asdict(r) for r in result]\n",
    "                    [f.write(json.dumps(r, ensure_ascii=False) + '\\n') for r in result]\n",
    "\n",
    "                elif isinstance(result, dict):\n",
    "                    f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n",
    "                    f.close()\n",
    "        \n",
    "                elif isinstance(result, Exception):\n",
    "                    with open(f'{filename}_error.jsonl', 'a') as f:\n",
    "                        f.write(json.dumps({'error': str(result)}, ensure_ascii=False) + '\\n')\n",
    "                        f.close()\n",
    "\n",
    "        else:\n",
    "            with open(f'{filename}_captcha.jsonl', 'a') as f:    \n",
    "                    f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n",
    "                    f.close()\n",
    "\n",
    "    def run(self, scrape_object: WebPage, scraper_function, filename: str):\n",
    "        # This function is used to run the scraper\n",
    "        self.driver  = uc.Chrome(headless=False,use_subprocess=False)\n",
    "        self.driver.get(scrape_object.link)\n",
    "        self.scroll_method(self.driver)\n",
    "        result =  scraper_function(self.driver, scrape_object)\n",
    "        self.write_to_jsonl(result, filename)\n",
    "        self.driver.quit()\n",
    "\n",
    "\n",
    "class Database:\n",
    "    def check_if_exists(link: str) -> bool:\n",
    "        # This function is used to check if the link already exists in the database\n",
    "        with open('israeli_times_links.jsonl', 'r') as f:\n",
    "            for line in f:\n",
    "                if json.loads(line)['link'] == link:\n",
    "                    return True\n",
    "        return False    \n",
    "\n",
    "\n",
    "class IsraeliTimesScraper():\n",
    "    def __init__(self):\n",
    "        self.scraper = Scraper()\n",
    "\n",
    "    def detect_type_article(self, link: str) -> str:\n",
    "        # This function is used to detect the type of article\n",
    "        if 'liveblog' in link:\n",
    "            return 'liveblog'\n",
    "        elif 'blogs.timesofisrael.com' in link:\n",
    "            return 'blog'\n",
    "        elif 'https://jewishchronicle.timesofisrael.com/' in link:\n",
    "            return 'jewishchronicle'\n",
    "        else:\n",
    "            return 'article'\n",
    "\n",
    "    def collect_page_titles(self, driver, _: WebPage) -> list[WebPage]:\n",
    "        result = []\n",
    "        h = driver.find_elements(By.XPATH, '//div[@class=\"headline\"]/a')\n",
    "        already_scraped_count =  0\n",
    "        for link in h:\n",
    "            href = link.get_attribute('href')\n",
    "            if Database.check_if_exists(href):\n",
    "                already_scraped_count += 1\n",
    "                continue\n",
    "\n",
    "            type_of_article = self.detect_type_article(href)\n",
    "\n",
    "            result.append(\n",
    "                WebPage(\n",
    "                website='timesofisrael',\n",
    "                url=driver.current_url,\n",
    "                date=None,\n",
    "                title=link.text,\n",
    "                link=href,\n",
    "                media_type=type_of_article,\n",
    "                content=None\n",
    "            )\n",
    "            )\n",
    "\n",
    "        print(\"Already scraped:\", already_scraped_count)\n",
    "        if len(result) == 0:\n",
    "            return Exception('Already scraped all articles on this page')\n",
    "        \n",
    "        print('Collecting:', len(result), 'articles from the page')\n",
    "\n",
    "        return result\n",
    "\n",
    "    def scrape_article(driver, scrape_object: WebPage) -> WebPage:\n",
    "        \"\"\"Scrapes title, content, date from the article page.\"\"\"\n",
    "        try:\n",
    "            title = driver.find_element(By.XPATH, '//h1[@class=\"headline\"]').text\n",
    "            content = driver.find_element(By.XPATH, '//div[@class=\"the-content\"]').text\n",
    "            date = driver.find_element(By.XPATH, '//span[@class=\"date\"]').text\n",
    "            return WebPage(\n",
    "                title =title,\n",
    "                date=date,\n",
    "                link=scrape_object.link,\n",
    "                type=scrape_object.type,\n",
    "                content=content\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        homepage = WebPage(link='https://www.timesofisrael.com/', media_type='homepage')\n",
    "        self.scraper.run(homepage, self.collect_page_titles, 'israeli_times_links')\n",
    "\n",
    "\n",
    "IsraeliTimesScraper().run()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c52586eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_type_article(link: str) -> str:\n",
    "    # This function is used to detect the type of article\n",
    "    if 'liveblog' in link:\n",
    "        return 'liveblog'\n",
    "    elif 'blogs.timesofisrael.com' in link:\n",
    "        return 'blog'\n",
    "    elif 'https://jewishchronicle.timesofisrael.com/' in link:\n",
    "        return 'jewishchronicle'\n",
    "    else:\n",
    "        return 'article'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ebfcc6",
   "metadata": {},
   "source": [
    "### First step is to collect URLs from the home page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2786d5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_page_titles() -> list[dict]:\n",
    "    result = []\n",
    "    h = driver.find_elements(By.XPATH, '//div[@class=\"headline\"]/a')\n",
    "    for link in h:\n",
    "        result.append({\n",
    "            'title': link.text, \n",
    "            'link': link.get_attribute('href'),\n",
    "            'type': detect_type_article(link.get_attribute('href')),\n",
    "        })\n",
    "    return result\n",
    "\n",
    "\n",
    "driver  = webdriver.Chrome()\n",
    "titles_to_scrape: list[dict] = collect_page_titles()\n",
    "with open('result.jsonl', 'w') as f:\n",
    "    for i in titles_to_scrape:\n",
    "        f.write(json.dumps(i, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4525cee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7054b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_method(driver):\n",
    "    \"\"\"Scrolls the page to the bottom\"\"\"\n",
    "    last_height = 0\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        else:\n",
    "            last_height = new_height\n",
    "\n",
    "\n",
    "def scrape_article(driver, scrape_object: dict) -> dict:\n",
    "    \"\"\"Scrapes title, content, date from the article page.\"\"\"\n",
    "    driver.get(scrape_object['link'])\n",
    "    try:\n",
    "        title = driver.find_element(By.XPATH, '//h1[@class=\"headline\"]').text\n",
    "        content = driver.find_element(By.XPATH, '//div[@class=\"the-content\"]').text\n",
    "        date = driver.find_element(By.XPATH, '//span[@class=\"date\"]').text\n",
    "        return {\n",
    "            'title': title,\n",
    "            'date': date,\n",
    "            'link': scrape_object['link'],\n",
    "            'type': scrape_object['type'],\n",
    "            'content': content\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def scraper_wrapper(scrape_object: dict, scraper_function: callable):\n",
    "    \"\"\"Wrapper for scraping web pages. Takes a scraper object and the function to scrape the page. \n",
    "    Writes the result of the scraper function to a jsonl file. \n",
    "    If scraper fails, likely due to captcha, the scrape_object is written to captcha.jsonl for debuging and retrials.\n",
    "    \"\"\"\n",
    "    driver  = uc.Chrome(headless=False,use_subprocess=False)\n",
    "    driver.get(scrape_object['link'])\n",
    "    scroll_method(driver)\n",
    "\n",
    "    result =  scraper_function(driver, scrape_object)\n",
    "    if result:\n",
    "        with open('test.jsonl', 'a') as f:    \n",
    "                f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n",
    "                f.close()\n",
    "    else:\n",
    "        with open('captchas.jsonl', 'a') as f:    \n",
    "                f.write(json.dumps(scrape_object, ensure_ascii=False) + '\\n')\n",
    "                f.close()\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "dicts = []\n",
    "with open('result.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        dicts.append(json.loads(line))\n",
    "    f.close()\n",
    "\n",
    "for i in dicts:\n",
    "    if i['type'] == 'article':\n",
    "         scraper_wrapper(i, scrape_article)\n",
    "    \n",
    "    time.sleep(randint(2, 5))\n",
    "    print(f\"Scraped {i['link']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7622566",
   "metadata": {},
   "source": [
    "```   author = driver.find_element(By.XPATH, '//span[@class=\"byline\"]/a').text\n",
    "```\n",
    "Note to self that author can have several <a> elements if more than one author"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
