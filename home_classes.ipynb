{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a4d5f6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import json\n",
    "import undetected_chromedriver as uc\n",
    "import time\n",
    "from random import randint\n",
    "import dataclasses as dc\n",
    "from datetime import datetime\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dd2002",
   "metadata": {},
   "source": [
    "## General functions\n",
    "\n",
    "Here we define a general function to scroll pages and detect type of article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f682976a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already scraped: 418\n",
      "Collecting: 19 articles from the page\n",
      "old date: 11 May 2025, 10:38 pm\n",
      "IDF issues ‘urgent’ evacuation warning for three Houthi-controlled ports in Yemen\n",
      "old date: 11 May 2025, 11:14 am\n",
      "Body of soldier Zvi Feldman, missing for 43 years, recovered from Syria by Mossad, IDF\n",
      "old date: 11 May 2025, 12:28 pm\n",
      "Purge of Hezbollah from Beirut airport satisfying Israeli, US officials — report\n",
      "old date: 11 May 2025, 10:38 pm\n",
      "IDF issues ‘urgent’ evacuation warning for three Houthi-controlled ports in Yemen\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 165\u001b[39m\n\u001b[32m    159\u001b[39m                     \u001b[38;5;28mself\u001b[39m.scraper.run(page, \u001b[38;5;28mself\u001b[39m.scrape_article, \u001b[33m'\u001b[39m\u001b[33misraeli_times_articles\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    161\u001b[39m                 time.sleep(randint(\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m \u001b[43mIsraeliTimesScraper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 159\u001b[39m, in \u001b[36mIsraeliTimesScraper.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    157\u001b[39m page = WebPage(link=page[\u001b[33m'\u001b[39m\u001b[33mlink\u001b[39m\u001b[33m'\u001b[39m], media_type=page[\u001b[33m'\u001b[39m\u001b[33mmedia_type\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m page.media_type == \u001b[33m'\u001b[39m\u001b[33marticle\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscraper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscrape_article\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43misraeli_times_articles\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m time.sleep(randint(\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mScraper.run\u001b[39m\u001b[34m(self, scrape_object, scraper_function, filename)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mself\u001b[39m.driver  = uc.Chrome(headless=\u001b[38;5;28;01mFalse\u001b[39;00m,use_subprocess=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28mself\u001b[39m.driver.get(scrape_object.link)\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscroll_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m result =  scraper_function(\u001b[38;5;28mself\u001b[39m.driver, scrape_object)\n\u001b[32m     62\u001b[39m \u001b[38;5;28mself\u001b[39m.write_to_jsonl(result, filename)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mScraper.scroll_method\u001b[39m\u001b[34m(self, driver)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     17\u001b[39m     driver.execute_script(\u001b[33m\"\u001b[39m\u001b[33mwindow.scrollTo(0, document.body.scrollHeight);\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     time.sleep(\u001b[32m1\u001b[39m)\n\u001b[32m     19\u001b[39m     new_height = driver.execute_script(\u001b[33m\"\u001b[39m\u001b[33mreturn document.body.scrollHeight\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m new_height == last_height:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "@dc.dataclass\n",
    "class WebPage:\n",
    "    website: str = None\n",
    "    url: str = None\n",
    "    link: str = None\n",
    "    title: str = None\n",
    "    media_type: str = None\n",
    "    date: str =None\n",
    "    content: str = None\n",
    "\n",
    "     \n",
    "class Scraper:\n",
    "        \n",
    "    def scroll_method(self, driver):\n",
    "        last_height = 0\n",
    "        while True:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(1)\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            else:\n",
    "                last_height = new_height\n",
    "\n",
    "    def write_to_jsonl(self, result: list[WebPage] | WebPage | Exception, filename: str):\n",
    "        \"\"\"Writes the result of the scraper function to a jsonl file. \n",
    "        If result is a list, each element is written to the file. \n",
    "        If result is a dict, it is written as a single line.\n",
    "        \"\"\"\n",
    "        if result:\n",
    "            with open(f'{filename}.jsonl', 'a') as f:        \n",
    "                if isinstance(result, list):\n",
    "                    result = [dc.asdict(r) for r in result]\n",
    "                    [f.write(json.dumps(r, ensure_ascii=False) + '\\n') for r in result]\n",
    "\n",
    "                elif isinstance(result, WebPage):\n",
    "                    print(result.title)\n",
    "                    f.write(json.dumps(dc.asdict(result), ensure_ascii=False) + '\\n')\n",
    "                    f.close()\n",
    "        \n",
    "                elif isinstance(result, Exception):\n",
    "                    with open(f'{filename}_error.jsonl', 'a') as f:\n",
    "                        f.write(json.dumps({'error': str(result)}, ensure_ascii=False) + '\\n')\n",
    "                        f.close()\n",
    "\n",
    "        else:\n",
    "            with open(f'{filename}_captcha.jsonl', 'a') as f:\n",
    "                if isinstance(result, list):\n",
    "                    for r in result:\n",
    "                        f.write(r + '\\n')\n",
    "                    f.close()\n",
    "                else:    \n",
    "                    f.write(result + '\\n')\n",
    "                    f.close()\n",
    "\n",
    "    def run(self, scrape_object: WebPage, scraper_function, filename: str):\n",
    "        # This function is used to run the scraper\n",
    "        self.driver  = uc.Chrome(headless=False,use_subprocess=False)\n",
    "        self.driver.get(scrape_object.link)\n",
    "        self.scroll_method(self.driver)\n",
    "        result =  scraper_function(self.driver, scrape_object)\n",
    "        self.write_to_jsonl(result, filename)\n",
    "        self.driver.quit()\n",
    "\n",
    "\n",
    "class Database:\n",
    "    def check_if_exists(link: str) -> bool:\n",
    "        # This function is used to check if the link already exists in the database\n",
    "        with open('israeli_times_links.jsonl', 'r') as f:\n",
    "            for line in f:\n",
    "                if json.loads(line)['link'] == link:\n",
    "                    return True\n",
    "        return False    \n",
    "\n",
    "\n",
    "class IsraeliTimesScraper():\n",
    "    def __init__(self):\n",
    "        self.scraper = Scraper()\n",
    "\n",
    "    def detect_type_article(self, link: str) -> str:\n",
    "        # This function is used to detect the type of article\n",
    "        if 'liveblog' in link:\n",
    "            return 'liveblog'\n",
    "        elif 'blogs.timesofisrael.com' in link:\n",
    "            return 'blog'\n",
    "        elif 'https://jewishchronicle.timesofisrael.com/' in link:\n",
    "            return 'jewishchronicle'\n",
    "        else:\n",
    "            return 'article'\n",
    "\n",
    "    def collect_page_titles(self, driver, _: WebPage) -> list[WebPage]:\n",
    "        result = []\n",
    "        h = driver.find_elements(By.XPATH, '//div[@class=\"headline\"]/a')\n",
    "        already_scraped_count =  0\n",
    "        for link in h:\n",
    "            href = link.get_attribute('href')\n",
    "            if Database.check_if_exists(href):\n",
    "                already_scraped_count += 1\n",
    "                continue\n",
    "\n",
    "            type_of_article = self.detect_type_article(href)\n",
    "\n",
    "            result.append(\n",
    "                WebPage(\n",
    "                website='timesofisrael',\n",
    "                url=driver.current_url,\n",
    "                date=None,\n",
    "                title=link.text,\n",
    "                link=href,\n",
    "                media_type=type_of_article,\n",
    "                content=None\n",
    "            )\n",
    "            )\n",
    "\n",
    "        print(\"Already scraped:\", already_scraped_count)\n",
    "        if len(result) == 0:\n",
    "            return Exception('Already scraped all articles on this page')\n",
    "        \n",
    "        print('Collecting:', len(result), 'articles from the page')\n",
    "\n",
    "        return result\n",
    "\n",
    "    def scrape_article(self, driver, scrape_object: WebPage) -> WebPage:\n",
    "        \"\"\"Scrapes title, content, date from the article page.\"\"\"\n",
    "        try:\n",
    "            title = driver.find_element(By.XPATH, '//h1[@class=\"headline\"]').text\n",
    "            content = driver.find_element(By.XPATH, '//div[@class=\"the-content\"]').text\n",
    "            date = driver.find_element(By.XPATH, '//span[@class=\"date\"]').text\n",
    "            article = WebPage(\n",
    "                website='timesofisrael',\n",
    "                title =title,\n",
    "                date=date,\n",
    "                link=scrape_object.link,\n",
    "                media_type=scrape_object.media_type,\n",
    "                content=content\n",
    "            )\n",
    "\n",
    "            print(\"old date:\", article.date)\n",
    "            if \"Today\" in article.date:\n",
    "                new_date = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "                print(\"new date\", new_date)\n",
    "                article.date = new_date\n",
    "                print(\"new date:\", article.date)\n",
    "\n",
    "            return article\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"'Error': {e}, 'title', {scrape_object.title}, 'link', {scrape_object.link} \\n\"\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        homepage = WebPage(link='https://www.timesofisrael.com/', media_type='homepage')\n",
    "        self.scraper.run(homepage, self.collect_page_titles, 'israeli_times_links')\n",
    "        with open('israeli_times_links.jsonl', 'r') as f:\n",
    "            for line in f:\n",
    "                page = json.loads(line)\n",
    "                page = WebPage(link=page['link'], media_type=page['media_type'])\n",
    "                if page.media_type == 'article':\n",
    "                    self.scraper.run(page, self.scrape_article, 'israeli_times_articles')\n",
    "                \n",
    "                time.sleep(randint(1, 3))\n",
    "\n",
    "\n",
    "\n",
    "IsraeliTimesScraper().run()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd2eb24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new date 2025-05-12\n"
     ]
    }
   ],
   "source": [
    "article_date = \"Today, 12:00 PM\"\n",
    "if \"Today\" in article_date:\n",
    "    new_date = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "    print(\"new date\", new_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef000ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping liveblog\n",
      "title: 72 content: 403 link: 72 date: 76\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "def collect_liveblog(driver, scrape_object: WebPage) -> list[WebPage]:\n",
    "    \"\"\"Scrapes title, content, date from the liveblog page.\"\"\"\n",
    "    print(\"Scraping liveblog\")\n",
    "    try:\n",
    "        title = driver.find_elements(By.XPATH, '//div[@class=\"liveblog-paragraph\"]//h4')\n",
    "        content = driver.find_elements(By.XPATH, '//div[@class=\"liveblog-paragraph\"]//p')\n",
    "        href = driver.find_elements(By.XPATH, '//div[@class=\"liveblog-paragraph\"]//h4//a')\n",
    "        dates = driver.find_elements(By.XPATH, '//div[@class=\"liveblog-date\"]//a//span')\n",
    "        result = []\n",
    "        print(\n",
    "            \"title:\", len(title), \n",
    "            \"content:\", len(content), \n",
    "            \"link:\", len(href), \n",
    "            \"date:\", len(dates)\n",
    "              )\n",
    "\n",
    "        for t, i, h, d in zip(title, content, href, dates): \n",
    "            print(\"Unziping\")\n",
    "            \n",
    "            title = t.text\n",
    "            content = ''.join(i.text)\n",
    "            href = h\n",
    "            timestamp = int(d.get_attribute('data-timestamp'))\n",
    "            dt_object = datetime.datetime.utcfromtimestamp(timestamp)\n",
    "            epoch = dt_object.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        \n",
    "            result.append(WebPage(\n",
    "                website='timesofisrael',\n",
    "                url=driver.current_url,\n",
    "                title =title,\n",
    "                date=epoch,\n",
    "                link=href.get_attribute('href'),\n",
    "                media_type='liveblog',\n",
    "                content=content\n",
    "            ))\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        return f\"'Error': {e}, 'title', {scrape_object.title}, 'link', {scrape_object.link} \\n\"\n",
    "    \n",
    "\n",
    "libeblog = WebPage(website='IsrealiTimes', link='https://www.timesofisrael.com/liveblog-may-11-2025/')\n",
    "\n",
    "Scraper().run(libeblog, collect_liveblog, 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
