{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d5f6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import json\n",
    "import undetected_chromedriver as uc\n",
    "import time\n",
    "from random import randint\n",
    "import dataclasses as dc\n",
    "from datetime import datetime\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dd2002",
   "metadata": {},
   "source": [
    "## General functions\n",
    "\n",
    "Here we define a general function to scroll pages and detect type of article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f682976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dc.dataclass\n",
    "class WebPage:\n",
    "    website: str = None\n",
    "    url: str = None\n",
    "    link: str = None\n",
    "    title: str = None\n",
    "    media_type: str = None\n",
    "    date: str =None\n",
    "    content: str = None\n",
    "   \n",
    "class Scraper:\n",
    "        \n",
    "    def scroll_method(self, driver):\n",
    "        try:\n",
    "            last_height = 0\n",
    "            while True:\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(1)\n",
    "                new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                if new_height == last_height:\n",
    "                    break\n",
    "                else:\n",
    "                    last_height = new_height\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "\n",
    "    def write_to_jsonl(self, result: list[WebPage] | WebPage | Exception, filename: str):\n",
    "        \"\"\"Writes the result of the scraper function to a jsonl file. \n",
    "        If result is a list, each element is written to the file. \n",
    "        If result is a dict, it is written as a single line.\n",
    "        \"\"\"\n",
    "        if result:\n",
    "            with open(f'output/{filename}.jsonl', 'a') as f:        \n",
    "                if isinstance(result, list):\n",
    "                    result = [dc.asdict(r) for r in result]\n",
    "                    [f.write(json.dumps(r, ensure_ascii=False) + '\\n') for r in result]\n",
    "\n",
    "                elif isinstance(result, WebPage):\n",
    "                    f.write(json.dumps(dc.asdict(result), ensure_ascii=False) + '\\n')\n",
    "                    f.close()\n",
    "        \n",
    "                elif isinstance(result, Exception):\n",
    "                    with open(f'logs/{filename}_error.jsonl', 'a') as f:\n",
    "                        f.write(json.dumps({'error': str(result)}, ensure_ascii=False) + '\\n')\n",
    "                        f.close()\n",
    "\n",
    "        else:\n",
    "            with open(f'logs/{filename}_captcha.jsonl', 'a') as f:\n",
    "                if isinstance(result, list):\n",
    "                    for r in result:\n",
    "                        f.write(r + '\\n')\n",
    "                    f.close()\n",
    "                else:    \n",
    "                    f.write(result + '\\n')\n",
    "                    f.close()\n",
    "\n",
    "    def run(self, scrape_object: WebPage, scraper_function, filename: str):\n",
    "        # This function is used to run the scraper\n",
    "        self.driver  = uc.Chrome(headless=False,use_subprocess=False)\n",
    "        self.driver.get(scrape_object.link)\n",
    "        self.scroll_method(self.driver)\n",
    "        result =  scraper_function(self.driver, scrape_object)\n",
    "        self.write_to_jsonl(result, filename)\n",
    "        self.driver.quit()\n",
    "\n",
    "\n",
    "class Database:\n",
    "    def check_if_exists(link: str) -> bool:\n",
    "        # This function is used to check if the link already exists in the database\n",
    "        with open('output/israeli_times_links.jsonl', 'r') as f:\n",
    "            for line in f:\n",
    "                if json.loads(line)['link'] == link:\n",
    "                    return True\n",
    "        return False    \n",
    "\n",
    "\n",
    "class IsraeliTimesScraper():\n",
    "    def __init__(self):\n",
    "        self.scraper = Scraper()\n",
    "\n",
    "    def detect_type_article(self, link: str) -> str:\n",
    "        # This function is used to detect the type of article\n",
    "        if 'liveblog' in link:\n",
    "            return 'liveblog'\n",
    "        elif 'blogs.timesofisrael.com' in link:\n",
    "            return 'blog'\n",
    "        elif 'https://jewishchronicle.timesofisrael.com/' in link:\n",
    "            return 'jewishchronicle'\n",
    "        else:\n",
    "            return 'article'\n",
    "\n",
    "    def collect_page_titles(self, driver, _: WebPage) -> list[WebPage]:\n",
    "        result = []\n",
    "        h = driver.find_elements(By.XPATH, '//div[@class=\"headline\"]/a')\n",
    "        already_scraped_count =  0\n",
    "        for link in h:\n",
    "            href = link.get_attribute('href')\n",
    "            if Database.check_if_exists(href):\n",
    "                already_scraped_count += 1\n",
    "                continue\n",
    "\n",
    "            type_of_article = self.detect_type_article(href)\n",
    "\n",
    "            result.append(\n",
    "                WebPage(\n",
    "                website='timesofisrael',\n",
    "                url=driver.current_url,\n",
    "                date=None,\n",
    "                title=link.text,\n",
    "                link=href,\n",
    "                media_type=type_of_article,\n",
    "                content=None\n",
    "            )\n",
    "            )\n",
    "        unique_domains = []\n",
    "        unique = set()\n",
    "        for r in result:\n",
    "            if r.link not in unique:\n",
    "                unique.add(r.link)\n",
    "                unique_domains.append(r)\n",
    "        print(\"Already scraped:\", already_scraped_count)\n",
    "        \n",
    "        if len(result) == 0:\n",
    "            return Exception('Already scraped all articles on this page')\n",
    "        \n",
    "        print('Collecting:', len(result), 'articles from the page')\n",
    "\n",
    "        return unique_domains\n",
    "\n",
    "    def scrape_article(self, driver, scrape_object: WebPage) -> WebPage:\n",
    "        \"\"\"Scrapes title, content, date from the article page.\"\"\"\n",
    "        try:\n",
    "            title = driver.find_element(By.XPATH, '//h1[@class=\"headline\"]').text\n",
    "            content = driver.find_element(By.XPATH, '//div[@class=\"the-content\"]').text\n",
    "            date = driver.find_element(By.XPATH, '//span[@class=\"date\"]').text\n",
    "            article = WebPage(\n",
    "                website='timesofisrael',\n",
    "                title =title,\n",
    "                date=date,\n",
    "                link=scrape_object.link,\n",
    "                media_type=scrape_object.media_type,\n",
    "                content=content\n",
    "            )\n",
    "\n",
    "            if \"Today\" in article.date:\n",
    "                new_date = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "                article.date = new_date\n",
    "\n",
    "            return article\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"'Error': {e}, 'title', {scrape_object.title}, 'link', {scrape_object.link} \\n\"\n",
    "\n",
    "    def collect_liveblog(self, driver, scrape_object: WebPage) -> list[WebPage]:\n",
    "        \"\"\"Scrapes title, content, date from the liveblog page.\"\"\"\n",
    "        try:\n",
    "            title = driver.find_elements(By.XPATH, '//div[@class=\"liveblog-paragraph\"]//h4')\n",
    "            content = driver.find_elements(By.XPATH, '//div[@class=\"liveblog-paragraph\"]//p')\n",
    "            href = driver.find_elements(By.XPATH, '//div[@class=\"liveblog-paragraph\"]//h4//a')\n",
    "            dates = driver.find_elements(By.XPATH, '//div[@class=\"liveblog-date\"]//a//span')\n",
    "            result = []\n",
    "            print(\n",
    "                \"title:\", len(title), \n",
    "                \"content:\", len(content), \n",
    "                \"link:\", len(href), \n",
    "                \"date:\", len(dates)\n",
    "                )\n",
    "\n",
    "            for t, i, h, d in zip(title, content, href, dates): \n",
    "                \n",
    "                # Convert epoch in timestamp to datetime\n",
    "                title = t.text\n",
    "                content = ''.join(i.text)\n",
    "                href = h\n",
    "                timestamp = int(d.get_attribute('data-timestamp'))\n",
    "                dt_object = datetime.datetime.utcfromtimestamp(timestamp)\n",
    "                epoch = dt_object.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            \n",
    "                result.append(WebPage(\n",
    "                    website='timesofisrael',\n",
    "                    url=driver.current_url,\n",
    "                    title =title,\n",
    "                    date=epoch,\n",
    "                    link=href.get_attribute('href'),\n",
    "                    media_type='liveblog',\n",
    "                    content=content\n",
    "                ))\n",
    "            \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "            return f\"'Error': {e}, 'title', {scrape_object.title}, 'link', {scrape_object.link} \\n\"\n",
    "\n",
    "    def collect_blogs(self, driver, scrape_object: WebPage) -> WebPage:\n",
    "        \"\"\"Scrapes title, content, date from the blog page.\"\"\"\n",
    "        try:\n",
    "            title = driver.find_element(By.XPATH, '//h1[@class=\"headline\"]').text\n",
    "            content = driver.find_element(By.XPATH, '//div[@class=\"article-content\"]').text\n",
    "            date = driver.find_element(By.XPATH, '//aside[@class=\"block cols1\"]//div[@class=\"date\"]').text\n",
    "            article = WebPage(\n",
    "                website='timesofisrael',\n",
    "                title =title,\n",
    "                date=date,\n",
    "                link=scrape_object.link,\n",
    "                media_type=scrape_object.media_type,\n",
    "                content=content\n",
    "            )  \n",
    "            return article\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"'Error': {e}, 'title', {scrape_object.title}, 'link', {scrape_object.link} \\n\"\n",
    "\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        homepage = WebPage(link='https://www.timesofisrael.com/', media_type='homepage')\n",
    "        self.scraper.run(homepage, self.collect_page_titles, 'israeli_times_links')\n",
    "        with open('output/israeli_times_links.jsonl', 'r') as f:\n",
    "            for line in f:\n",
    "                page = json.loads(line)\n",
    "                page = WebPage(link=page['link'], media_type=page['media_type'])\n",
    "                if page.media_type == 'article':\n",
    "                    self.scraper.run(page, self.scrape_article, 'data')\n",
    "                elif page.media_type == 'liveblog':\n",
    "                    self.scraper.run(page, self.collect_liveblog, 'data')\n",
    "                elif page.media_type == 'blog':\n",
    "                    self.scraper.run(page, self.collect_blogs, 'data')\n",
    "                \n",
    "                \n",
    "                time.sleep(randint(1, 3))\n",
    "\n",
    "\n",
    "\n",
    "IsraeliTimesScraper().run()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87c58c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_blogs(driver, scrape_object: WebPage) -> WebPage:\n",
    "    \"\"\"Scrapes title, content, date from the blog page.\"\"\"\n",
    "    try:\n",
    "        title = driver.find_element(By.XPATH, '//h1[@class=\"headline\"]').text\n",
    "        content = driver.find_element(By.XPATH, '//div[@class=\"article-content\"]').text\n",
    "        date = driver.find_element(By.XPATH, '//aside[@class=\"block cols1\"]//div[@class=\"date\"]').text\n",
    "        article = WebPage(\n",
    "            website='timesofisrael',\n",
    "            title =title,\n",
    "            date=date,\n",
    "            link=scrape_object.link,\n",
    "            media_type=scrape_object.media_type,\n",
    "            content=content\n",
    "        )  \n",
    "        return article\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"'Error': {e}, 'title', {scrape_object.title}, 'link', {scrape_object.link} \\n\"\n",
    "\n",
    "\n",
    "blog = WebPage(link='https://blogs.timesofisrael.com/when-non-jews-help-jews/', media_type='blog')\n",
    "IsraeliTimesScraper().scraper.run(blog, collect_blogs, 'blog_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c655647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
