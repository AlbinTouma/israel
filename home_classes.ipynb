{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a4d5f6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import json\n",
    "import undetected_chromedriver as uc\n",
    "import time\n",
    "from random import randint\n",
    "import dataclasses as dc\n",
    "from datetime import datetime\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dd2002",
   "metadata": {},
   "source": [
    "## General functions\n",
    "\n",
    "Here we define a general function to scroll pages and detect type of article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f682976a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already scraped: 395\n",
      "Collecting: 50 articles from the page\n",
      "Scraping liveblog\n",
      "title: 10 content: 29 link: 10 date: 18\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "old date: Today, 2:34 pm\n",
      "new date 2025-05-12\n",
      "new date: 2025-05-12\n",
      "IDF said limiting Gaza operations ahead of Edan Alexander release, but no truce\n",
      "old date: Today, 3:22 am\n",
      "new date 2025-05-12\n",
      "new date: 2025-05-12\n",
      "Hamas set to free US-Israeli hostage Edan Alexander Monday in goodwill gesture to Trump\n",
      "old date: 26 February 2025, 10:32 am\n",
      "‘A bigger fight than anything imagined’: Edan Alexander’s mom battles for hostage son\n",
      "old date: Today, 7:36 am\n",
      "new date 2025-05-12\n",
      "new date: 2025-05-12\n",
      "Backchannel helped secure release of American-Israeli hostage, sources tell ToI\n",
      "Scraping liveblog\n",
      "title: 10 content: 29 link: 10 date: 18\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "old date: Today, 2:34 pm\n",
      "new date 2025-05-12\n",
      "new date: 2025-05-12\n",
      "IDF said limiting Gaza operations ahead of Edan Alexander release, but no truce\n",
      "old date: Today, 3:22 am\n",
      "new date 2025-05-12\n",
      "new date: 2025-05-12\n",
      "Hamas set to free US-Israeli hostage Edan Alexander Monday in goodwill gesture to Trump\n",
      "old date: 26 February 2025, 10:32 am\n",
      "‘A bigger fight than anything imagined’: Edan Alexander’s mom battles for hostage son\n",
      "old date: Today, 7:36 am\n",
      "new date 2025-05-12\n",
      "new date: 2025-05-12\n",
      "Backchannel helped secure release of American-Israeli hostage, sources tell ToI\n",
      "old date: Today, 8:10 pm\n",
      "new date 2025-05-12\n",
      "new date: 2025-05-12\n",
      "As Edan Alexander freed, opposition chiefs accuse PM of bungling ties with Washington\n",
      "old date: Today, 9:52 am\n",
      "new date 2025-05-12\n",
      "new date: 2025-05-12\n",
      "Netanyahu insists ties with Trump are ‘excellent’ amid reports of growing rift\n",
      "Scraping liveblog\n",
      "title: 72 content: 403 link: 72 date: 76\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "Unziping\n",
      "old date: 11 May 2025, 2:49 pm\n",
      "Saudi-Israel deal remains elusive as Trump visits in bid for $1 trillion bonanza\n",
      "old date: Today, 11:41 am\n",
      "new date 2025-05-12\n",
      "new date: 2025-05-12\n",
      "For families, joy, despair, frustration over coming release of hostage Edan Alexander\n",
      "old date: 10 May 2025, 11:03 pm\n",
      "Hostages’ families warn of ‘lost opportunity of the century’ as Hamas airs new clip of captives\n",
      "old date: Today, 12:57 am\n",
      "new date 2025-05-12\n",
      "new date: 2025-05-12\n",
      "Witkoff said to tell hostage families Israel pointlessly extending war, US urging deal\n",
      "old date: Today, 12:40 pm\n",
      "new date 2025-05-12\n",
      "new date: 2025-05-12\n",
      "In Israel, German FM says Gaza war cannot be solved by ‘military means,’ urges deal\n",
      "old date: Today, 5:04 pm\n",
      "new date 2025-05-12\n",
      "new date: 2025-05-12\n",
      "Gaza at risk of famine if Israel doesn’t lift ban on aid — food security group\n",
      "old date: Today, 5:38 am\n",
      "new date 2025-05-12\n",
      "new date: 2025-05-12\n",
      "Sa’ar backs newly announced ‘US aid plan’ to bypass Hamas in Gaza\n",
      "old date: Today, 10:32 pm\n",
      "new date 2025-05-12\n",
      "new date: 2025-05-12\n",
      "Kyiv rabbi seeks IDF guidance to avoid cremating Jewish soldiers in Ukraine\n",
      "old date: Today, 9:44 pm\n",
      "new date 2025-05-12\n",
      "new date: 2025-05-12\n",
      "Archaeologists launch new excavation in West Bank at capital of ancient Israel\n",
      "old date: Today, 9:01 pm\n",
      "new date 2025-05-12\n",
      "new date: 2025-05-12\n",
      "Renegade Shas rabbi to establish new party that supports Haredi conscription\n",
      "old date: Today, 8:10 pm\n",
      "new date 2025-05-12\n",
      "new date: 2025-05-12\n",
      "As Edan Alexander freed, opposition chiefs accuse PM of bungling ties with Washington\n"
     ]
    },
    {
     "ename": "InvalidSessionIdException",
     "evalue": "Message: invalid session id: session deleted as the browser has closed the connection\nfrom disconnected: not connected to DevTools\n  (Session info: chrome=136.0.7103.59)\nStacktrace:\n#0 0x5d63b3bd278a <unknown>\n#1 0x5d63b36750a0 <unknown>\n#2 0x5d63b365b8ae <unknown>\n#3 0x5d63b3683be9 <unknown>\n#4 0x5d63b36f4cef <unknown>\n#5 0x5d63b3711c3c <unknown>\n#6 0x5d63b36ec173 <unknown>\n#7 0x5d63b36b8d4b <unknown>\n#8 0x5d63b36b99b1 <unknown>\n#9 0x5d63b3b9793b <unknown>\n#10 0x5d63b3b9b83a <unknown>\n#11 0x5d63b3b7f692 <unknown>\n#12 0x5d63b3b9c3c4 <unknown>\n#13 0x5d63b3b644cf <unknown>\n#14 0x5d63b3bc0568 <unknown>\n#15 0x5d63b3bc0746 <unknown>\n#16 0x5d63b3bd15f6 <unknown>\n#17 0x7d859cc9caa4 <unknown>\n#18 0x7d859cd29c3c <unknown>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidSessionIdException\u001b[39m                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 215\u001b[39m\n\u001b[32m    209\u001b[39m                     \u001b[38;5;28mself\u001b[39m.scraper.run(page, \u001b[38;5;28mself\u001b[39m.collect_liveblog, \u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    211\u001b[39m                 time.sleep(randint(\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m \u001b[43mIsraeliTimesScraper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 207\u001b[39m, in \u001b[36mIsraeliTimesScraper.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    205\u001b[39m page = WebPage(link=page[\u001b[33m'\u001b[39m\u001b[33mlink\u001b[39m\u001b[33m'\u001b[39m], media_type=page[\u001b[33m'\u001b[39m\u001b[33mmedia_type\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m page.media_type == \u001b[33m'\u001b[39m\u001b[33marticle\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscraper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscrape_article\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m page.media_type == \u001b[33m'\u001b[39m\u001b[33mliveblog\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    209\u001b[39m     \u001b[38;5;28mself\u001b[39m.scraper.run(page, \u001b[38;5;28mself\u001b[39m.collect_liveblog, \u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mScraper.run\u001b[39m\u001b[34m(self, scrape_object, scraper_function, filename)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mself\u001b[39m.driver  = uc.Chrome(headless=\u001b[38;5;28;01mFalse\u001b[39;00m,use_subprocess=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28mself\u001b[39m.driver.get(scrape_object.link)\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscroll_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m result =  scraper_function(\u001b[38;5;28mself\u001b[39m.driver, scrape_object)\n\u001b[32m     62\u001b[39m \u001b[38;5;28mself\u001b[39m.write_to_jsonl(result, filename)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mScraper.scroll_method\u001b[39m\u001b[34m(self, driver)\u001b[39m\n\u001b[32m     17\u001b[39m driver.execute_script(\u001b[33m\"\u001b[39m\u001b[33mwindow.scrollTo(0, document.body.scrollHeight);\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m time.sleep(\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m new_height = \u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_script\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreturn document.body.scrollHeight\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m new_height == last_height:\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/israel/.env/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:547\u001b[39m, in \u001b[36mWebDriver.execute_script\u001b[39m\u001b[34m(self, script, *args)\u001b[39m\n\u001b[32m    544\u001b[39m converted_args = \u001b[38;5;28mlist\u001b[39m(args)\n\u001b[32m    545\u001b[39m command = Command.W3C_EXECUTE_SCRIPT\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscript\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscript\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43margs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverted_args\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/israel/.env/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:448\u001b[39m, in \u001b[36mWebDriver.execute\u001b[39m\u001b[34m(self, driver_command, params)\u001b[39m\n\u001b[32m    446\u001b[39m response = \u001b[38;5;28mself\u001b[39m.command_executor.execute(driver_command, params)\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merror_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    449\u001b[39m     response[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._unwrap_value(response.get(\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    450\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/israel/.env/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py:232\u001b[39m, in \u001b[36mErrorHandler.check_response\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m    230\u001b[39m         alert_text = value[\u001b[33m\"\u001b[39m\u001b[33malert\u001b[39m\u001b[33m\"\u001b[39m].get(\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    231\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[31mInvalidSessionIdException\u001b[39m: Message: invalid session id: session deleted as the browser has closed the connection\nfrom disconnected: not connected to DevTools\n  (Session info: chrome=136.0.7103.59)\nStacktrace:\n#0 0x5d63b3bd278a <unknown>\n#1 0x5d63b36750a0 <unknown>\n#2 0x5d63b365b8ae <unknown>\n#3 0x5d63b3683be9 <unknown>\n#4 0x5d63b36f4cef <unknown>\n#5 0x5d63b3711c3c <unknown>\n#6 0x5d63b36ec173 <unknown>\n#7 0x5d63b36b8d4b <unknown>\n#8 0x5d63b36b99b1 <unknown>\n#9 0x5d63b3b9793b <unknown>\n#10 0x5d63b3b9b83a <unknown>\n#11 0x5d63b3b7f692 <unknown>\n#12 0x5d63b3b9c3c4 <unknown>\n#13 0x5d63b3b644cf <unknown>\n#14 0x5d63b3bc0568 <unknown>\n#15 0x5d63b3bc0746 <unknown>\n#16 0x5d63b3bd15f6 <unknown>\n#17 0x7d859cc9caa4 <unknown>\n#18 0x7d859cd29c3c <unknown>\n"
     ]
    }
   ],
   "source": [
    "@dc.dataclass\n",
    "class WebPage:\n",
    "    website: str = None\n",
    "    url: str = None\n",
    "    link: str = None\n",
    "    title: str = None\n",
    "    media_type: str = None\n",
    "    date: str =None\n",
    "    content: str = None\n",
    "\n",
    "     \n",
    "class Scraper:\n",
    "        \n",
    "    def scroll_method(self, driver):\n",
    "        last_height = 0\n",
    "        while True:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(1)\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            else:\n",
    "                last_height = new_height\n",
    "\n",
    "    def write_to_jsonl(self, result: list[WebPage] | WebPage | Exception, filename: str):\n",
    "        \"\"\"Writes the result of the scraper function to a jsonl file. \n",
    "        If result is a list, each element is written to the file. \n",
    "        If result is a dict, it is written as a single line.\n",
    "        \"\"\"\n",
    "        if result:\n",
    "            with open(f'output/{filename}.jsonl', 'a') as f:        \n",
    "                if isinstance(result, list):\n",
    "                    result = [dc.asdict(r) for r in result]\n",
    "                    [f.write(json.dumps(r, ensure_ascii=False) + '\\n') for r in result]\n",
    "\n",
    "                elif isinstance(result, WebPage):\n",
    "                    print(result.title)\n",
    "                    f.write(json.dumps(dc.asdict(result), ensure_ascii=False) + '\\n')\n",
    "                    f.close()\n",
    "        \n",
    "                elif isinstance(result, Exception):\n",
    "                    with open(f'logs/{filename}_error.jsonl', 'a') as f:\n",
    "                        f.write(json.dumps({'error': str(result)}, ensure_ascii=False) + '\\n')\n",
    "                        f.close()\n",
    "\n",
    "        else:\n",
    "            with open(f'logs/{filename}_captcha.jsonl', 'a') as f:\n",
    "                if isinstance(result, list):\n",
    "                    for r in result:\n",
    "                        f.write(r + '\\n')\n",
    "                    f.close()\n",
    "                else:    \n",
    "                    f.write(result + '\\n')\n",
    "                    f.close()\n",
    "\n",
    "    def run(self, scrape_object: WebPage, scraper_function, filename: str):\n",
    "        # This function is used to run the scraper\n",
    "        self.driver  = uc.Chrome(headless=False,use_subprocess=False)\n",
    "        self.driver.get(scrape_object.link)\n",
    "        self.scroll_method(self.driver)\n",
    "        result =  scraper_function(self.driver, scrape_object)\n",
    "        self.write_to_jsonl(result, filename)\n",
    "        self.driver.quit()\n",
    "\n",
    "\n",
    "class Database:\n",
    "    def check_if_exists(link: str) -> bool:\n",
    "        # This function is used to check if the link already exists in the database\n",
    "        with open('output/israeli_times_links.jsonl', 'r') as f:\n",
    "            for line in f:\n",
    "                if json.loads(line)['link'] == link:\n",
    "                    return True\n",
    "        return False    \n",
    "\n",
    "\n",
    "class IsraeliTimesScraper():\n",
    "    def __init__(self):\n",
    "        self.scraper = Scraper()\n",
    "\n",
    "    def detect_type_article(self, link: str) -> str:\n",
    "        # This function is used to detect the type of article\n",
    "        if 'liveblog' in link:\n",
    "            return 'liveblog'\n",
    "        elif 'blogs.timesofisrael.com' in link:\n",
    "            return 'blog'\n",
    "        elif 'https://jewishchronicle.timesofisrael.com/' in link:\n",
    "            return 'jewishchronicle'\n",
    "        else:\n",
    "            return 'article'\n",
    "\n",
    "    def collect_page_titles(self, driver, _: WebPage) -> list[WebPage]:\n",
    "        result = []\n",
    "        h = driver.find_elements(By.XPATH, '//div[@class=\"headline\"]/a')\n",
    "        already_scraped_count =  0\n",
    "        for link in h:\n",
    "            href = link.get_attribute('href')\n",
    "            if Database.check_if_exists(href):\n",
    "                already_scraped_count += 1\n",
    "                continue\n",
    "\n",
    "            type_of_article = self.detect_type_article(href)\n",
    "\n",
    "            result.append(\n",
    "                WebPage(\n",
    "                website='timesofisrael',\n",
    "                url=driver.current_url,\n",
    "                date=None,\n",
    "                title=link.text,\n",
    "                link=href,\n",
    "                media_type=type_of_article,\n",
    "                content=None\n",
    "            )\n",
    "            )\n",
    "        unique_domains = []\n",
    "        unique = set()\n",
    "        for r in result:\n",
    "            if r.link not in unique:\n",
    "                unique.add(r.link)\n",
    "                unique_domains.append(r)\n",
    "\n",
    "        print(\"Already scraped:\", already_scraped_count)\n",
    "        if len(result) == 0:\n",
    "            return Exception('Already scraped all articles on this page')\n",
    "        \n",
    "        print('Collecting:', len(result), 'articles from the page')\n",
    "\n",
    "        return unique_domains\n",
    "\n",
    "    def scrape_article(self, driver, scrape_object: WebPage) -> WebPage:\n",
    "        \"\"\"Scrapes title, content, date from the article page.\"\"\"\n",
    "        try:\n",
    "            title = driver.find_element(By.XPATH, '//h1[@class=\"headline\"]').text\n",
    "            content = driver.find_element(By.XPATH, '//div[@class=\"the-content\"]').text\n",
    "            date = driver.find_element(By.XPATH, '//span[@class=\"date\"]').text\n",
    "            article = WebPage(\n",
    "                website='timesofisrael',\n",
    "                title =title,\n",
    "                date=date,\n",
    "                link=scrape_object.link,\n",
    "                media_type=scrape_object.media_type,\n",
    "                content=content\n",
    "            )\n",
    "\n",
    "            print(\"old date:\", article.date)\n",
    "            if \"Today\" in article.date:\n",
    "                new_date = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "                print(\"new date\", new_date)\n",
    "                article.date = new_date\n",
    "                print(\"new date:\", article.date)\n",
    "\n",
    "            return article\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"'Error': {e}, 'title', {scrape_object.title}, 'link', {scrape_object.link} \\n\"\n",
    "\n",
    "    def collect_liveblog(self, driver, scrape_object: WebPage) -> list[WebPage]:\n",
    "        \"\"\"Scrapes title, content, date from the liveblog page.\"\"\"\n",
    "        print(\"Scraping liveblog\")\n",
    "        try:\n",
    "            title = driver.find_elements(By.XPATH, '//div[@class=\"liveblog-paragraph\"]//h4')\n",
    "            content = driver.find_elements(By.XPATH, '//div[@class=\"liveblog-paragraph\"]//p')\n",
    "            href = driver.find_elements(By.XPATH, '//div[@class=\"liveblog-paragraph\"]//h4//a')\n",
    "            dates = driver.find_elements(By.XPATH, '//div[@class=\"liveblog-date\"]//a//span')\n",
    "            result = []\n",
    "            print(\n",
    "                \"title:\", len(title), \n",
    "                \"content:\", len(content), \n",
    "                \"link:\", len(href), \n",
    "                \"date:\", len(dates)\n",
    "                )\n",
    "\n",
    "            for t, i, h, d in zip(title, content, href, dates): \n",
    "                print(\"Unziping\")\n",
    "                \n",
    "                # Convert epoch in timestamp to datetime\n",
    "                title = t.text\n",
    "                content = ''.join(i.text)\n",
    "                href = h\n",
    "                timestamp = int(d.get_attribute('data-timestamp'))\n",
    "                dt_object = datetime.datetime.utcfromtimestamp(timestamp)\n",
    "                epoch = dt_object.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            \n",
    "                result.append(WebPage(\n",
    "                    website='timesofisrael',\n",
    "                    url=driver.current_url,\n",
    "                    title =title,\n",
    "                    date=epoch,\n",
    "                    link=href.get_attribute('href'),\n",
    "                    media_type='liveblog',\n",
    "                    content=content\n",
    "                ))\n",
    "            \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "            return f\"'Error': {e}, 'title', {scrape_object.title}, 'link', {scrape_object.link} \\n\"\n",
    "        \n",
    "    def run(self):\n",
    "        homepage = WebPage(link='https://www.timesofisrael.com/', media_type='homepage')\n",
    "        self.scraper.run(homepage, self.collect_page_titles, 'israeli_times_links')\n",
    "        with open('output/israeli_times_links.jsonl', 'r') as f:\n",
    "            for line in f:\n",
    "                page = json.loads(line)\n",
    "                page = WebPage(link=page['link'], media_type=page['media_type'])\n",
    "                if page.media_type == 'article':\n",
    "                    self.scraper.run(page, self.scrape_article, 'data')\n",
    "                elif page.media_type == 'liveblog':\n",
    "                    self.scraper.run(page, self.collect_liveblog, 'data')\n",
    "                \n",
    "                time.sleep(randint(1, 3))\n",
    "\n",
    "\n",
    "\n",
    "IsraeliTimesScraper().run()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1e6796",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef000ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping liveblog\n",
      "title: 72 content: 403 link: 72 date: 76\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n",
      "Unziping\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "def collect_liveblog(driver, scrape_object: WebPage) -> list[WebPage]:\n",
    "    \"\"\"Scrapes title, content, date from the liveblog page.\"\"\"\n",
    "    print(\"Scraping liveblog\")\n",
    "    try:\n",
    "        title = driver.find_elements(By.XPATH, '//div[@class=\"liveblog-paragraph\"]//h4')\n",
    "        content = driver.find_elements(By.XPATH, '//div[@class=\"liveblog-paragraph\"]//p')\n",
    "        href = driver.find_elements(By.XPATH, '//div[@class=\"liveblog-paragraph\"]//h4//a')\n",
    "        dates = driver.find_elements(By.XPATH, '//div[@class=\"liveblog-date\"]//a//span')\n",
    "        result = []\n",
    "        print(\n",
    "            \"title:\", len(title), \n",
    "            \"content:\", len(content), \n",
    "            \"link:\", len(href), \n",
    "            \"date:\", len(dates)\n",
    "              )\n",
    "\n",
    "        for t, i, h, d in zip(title, content, href, dates): \n",
    "            print(\"Unziping\")\n",
    "            \n",
    "            title = t.text\n",
    "            content = ''.join(i.text)\n",
    "            href = h\n",
    "            timestamp = int(d.get_attribute('data-timestamp'))\n",
    "            dt_object = datetime.datetime.utcfromtimestamp(timestamp)\n",
    "            epoch = dt_object.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        \n",
    "            result.append(WebPage(\n",
    "                website='timesofisrael',\n",
    "                url=driver.current_url,\n",
    "                title =title,\n",
    "                date=epoch,\n",
    "                link=href.get_attribute('href'),\n",
    "                media_type='liveblog',\n",
    "                content=content\n",
    "            ))\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        return f\"'Error': {e}, 'title', {scrape_object.title}, 'link', {scrape_object.link} \\n\"\n",
    "    \n",
    "\n",
    "libeblog = WebPage(website='IsrealiTimes', link='https://www.timesofisrael.com/liveblog-may-11-2025/')\n",
    "\n",
    "Scraper().run(libeblog, collect_liveblog, 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
